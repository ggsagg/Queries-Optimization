{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36msparksession._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._, func._\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.sparksession\n",
    "import sparksession._\n",
    "import spark.implicits._\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._, func._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On `DataFrame`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create datasets from external data sources using different formats, e.g. Json, parquet, CSV, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">json at cmd8.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpeople\u001b[39m: \u001b[32mDataFrame\u001b[39m = [age: bigint, name: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val people: DataFrame = spark.read.json(\"data/people.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we created a `DataFrame`, not a `Dataset`. Dataframes are like datasets, i.e. programs to generate distributed data sets, but *dynamically typed*. This means that, at compile time, Scala only knows that a dataframe consists of `Row`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd9.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres9\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [2000,YiHui],\n",
       "  [2000,Javier],\n",
       "  [2003,Gabriel],\n",
       "  [2004,Noelia]\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, a `DataFrame` is defined as an alias of `Dataset`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpeopleDs\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [age: bigint, name: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val peopleDs: Dataset[Row] = people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the type of the information to be processed is there! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11_0\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"age\"\u001b[39m, LongType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"name\"\u001b[39m, StringType, true, {})\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.schema\n",
    "people.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can convert a dataframe into a dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mPerson\u001b[39m\n",
       "\u001b[36mpeopleDs\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mPerson\u001b[39m] = [age: bigint, name: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "val peopleDs: Dataset[Person] = people.as[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd13.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|2000|  YiHui|\n",
      "|2000| Javier|\n",
      "|2003|Gabriel|\n",
      "|2004| Noelia|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd13.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|2000|  YiHui|\n",
      "|2000| Javier|\n",
      "|2003|Gabriel|\n",
      "|2004| Noelia|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDs.show\n",
    "people.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untyped transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` API includes a section on _untyped transformations_. These are transformations that are not defined over the Scala types but over the inner Spark SQL types (i.e. `StructType`s). More exactly, these could be named *dynamically typed transformations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transformations are in close corresponde with their SQL counterparts: `SELECT`, `WHERE`, `GROUP BY`, `FROM`, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `select` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the equivalent to the `map` typed transformation is `select`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd14.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd14.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|  YiHui|\n",
      "| Javier|\n",
      "|Gabriel|\n",
      "| Noelia|\n",
      "+-------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false) AS value#60]\n",
      "+- *(1) MapElements ammonite.$sess.cmd14$Helper$$Lambda$5098/1786149424@1e8583f5, obj#59: java.lang.String\n",
      "   +- *(1) DeserializeToObject newInstance(class ammonite.$sess.cmd12$Helper$Person), obj#58: ammonite.$sess.cmd12$Helper$Person\n",
      "      +- *(1) FileScan json [age#29L,name#30] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/home/jovyan/work/data/people.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint,name:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mds\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mString\u001b[39m] = [value: string]\n",
       "\u001b[36mres14_1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"YiHui\"\u001b[39m, \u001b[32m\"Javier\"\u001b[39m, \u001b[32m\"Gabriel\"\u001b[39m, \u001b[32m\"Noelia\"\u001b[39m)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds: Dataset[String] = peopleDs.map(_.name)\n",
    "ds.collect\n",
    "ds.show\n",
    "ds.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">json at cmd15.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd15.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd15.sc:4</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|  YiHui|\n",
      "| Javier|\n",
      "|Gabriel|\n",
      "| Noelia|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string]\n",
       "\u001b[36mres15_1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m([YiHui], [Javier], [Gabriel], [Noelia])\n",
       "\u001b[36mres15_3\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\u001b[33mStructField\u001b[39m(\u001b[32m\"name\"\u001b[39m, StringType, true, {}))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df: DataFrame = \n",
    "    spark.read.json(\"data/people.json\").select($\"name\")\n",
    "df.collect\n",
    "df.show\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we lost the column label (`name`) in the case of the dataset transformation. This is not happening with `select`. Moreover, we have more control over the resulting schema: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd16.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+\n",
      "|     _1|  _2| _3|\n",
      "+-------+----+---+\n",
      "|  YiHui|2001|YiH|\n",
      "| Javier|2001|Jav|\n",
      "|Gabriel|2004|Gab|\n",
      "| Noelia|2005|Noe|\n",
      "+-------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDs.map(p => (p.name, p.age + 1, p.name.substring(0,3)))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd17.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|prefix|\n",
      "+-------+----+------+\n",
      "|  YiHui|2001|   YiH|\n",
      "| Javier|2001|   Jav|\n",
      "|Gabriel|2004|   Gab|\n",
      "| Noelia|2005|   Noe|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select($\"name\", $\"age\" + 1 as \"age\", $\"name\".substr(0,3) as \"prefix\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) contains dozens of column operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that _untyped_, or more properly, _dynamically typed_, character means that the Scala compiler won't complain if we choose a non-existent column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"jp-RenderedText\">\n",
       "<pre><code><span style=\"color: rgb(0, 187, 187)\"><span class=\"ansi-cyan-fg\">df</span></span>: <span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">DataFrame</span></span> = <style>@keyframes fadein { from { opacity: 0; } to { opacity: 1; } }</style><span style=\"animation: fadein 2s;\"><span style=\"color: white\"><span class=\"ansi-white-fg\">[last attempt failed]</span></span>\n",
       "<span style=\"color: red\"><span class=\"ansi-red-fg\">org.apache.spark.sql.AnalysisException: cannot resolve '`nam`' given input columns: [age, name];;\n",
       "'Project ['nam]\n",
       "+- Relation[age#133L,name#134] json\n",
       "</span></span>\n",
       "  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">package.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">42</span></span>)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">CheckAnalysis.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">110</span></span>)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">CheckAnalysis.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">107</span></span>)\n",
       "  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">TreeNode.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">278</span></span>)\n",
       "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">TreeNode.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">70</span></span>)\n",
       "  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">TreeNode.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">278</span></span>)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryPlan.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">93</span></span>)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryPlan.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">105</span></span>)\n",
       "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">TreeNode.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">70</span></span>)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryPlan.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">105</span></span>)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryPlan.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">116</span></span>)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryPlan.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">121</span></span>)\n",
       "  scala.collection.TraversableLike.$anonfun$map$1(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">TraversableLike.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">238</span></span>)\n",
       "  scala.collection.mutable.ResizableArray.foreach(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">ResizableArray.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">62</span></span>)\n",
       "  scala.collection.mutable.ResizableArray.foreach$(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">ResizableArray.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">55</span></span>)\n",
       "  scala.collection.mutable.ArrayBuffer.foreach(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">ArrayBuffer.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">49</span></span>)\n",
       "  scala.collection.TraversableLike.map(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">TraversableLike.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">238</span></span>)\n",
       "  scala.collection.TraversableLike.map$(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">TraversableLike.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">231</span></span>)\n",
       "  scala.collection.AbstractTraversable.map(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Traversable.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">108</span></span>)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryPlan.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">121</span></span>)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryPlan.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">126</span></span>)\n",
       "  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">TreeNode.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">187</span></span>)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryPlan.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">126</span></span>)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryPlan.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">93</span></span>)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">CheckAnalysis.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">107</span></span>)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">CheckAnalysis.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">85</span></span>)\n",
       "  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">TreeNode.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">127</span></span>)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">CheckAnalysis.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">85</span></span>)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">CheckAnalysis.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">82</span></span>)\n",
       "  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Analyzer.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">95</span></span>)\n",
       "  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Analyzer.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">108</span></span>)\n",
       "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">AnalysisHelper.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">201</span></span>)\n",
       "  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Analyzer.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">105</span></span>)\n",
       "  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryExecution.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">57</span></span>)\n",
       "  org.apache.spark.sql.execution.QueryExecution.analyzed(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryExecution.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">55</span></span>)\n",
       "  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">QueryExecution.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">47</span></span>)\n",
       "  org.apache.spark.sql.Dataset$.ofRows(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Dataset.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">78</span></span>)\n",
       "  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Dataset.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">3406</span></span>)\n",
       "  org.apache.spark.sql.Dataset.select(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Dataset.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1335</span></span>)\n",
       "  ammonite.$sess.cmd18$Helper.$anonfun$df$value$1(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">cmd18.sc</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1</span></span>)\n",
       "  almond.api.internal.Lazy.liftedTree1$1(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Lazy.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">10</span></span>)\n",
       "  almond.api.internal.Lazy.value$lzycompute(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Lazy.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">10</span></span>)\n",
       "  almond.api.internal.Lazy.value(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">Lazy.scala</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">8</span></span>)\n",
       "  ammonite.$sess.cmd19$Helper.&lt;init&gt;(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">cmd19.sc</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">1</span></span>)\n",
       "  ammonite.$sess.cmd19$.&lt;init&gt;(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">cmd19.sc</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">7</span></span>)\n",
       "  ammonite.$sess.cmd19$.&lt;clinit&gt;(<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">cmd19.sc</span></span>:<span style=\"color: rgb(0, 187, 0)\"><span class=\"ansi-green-fg\">-1</span></span>)</span></code></pre>\n",
       "</div>"
      ],
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = \u001b[37m[last attempt failed]\u001b[39m\n",
       "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`nam`' given input columns: [age, name];;\n",
       "'Project ['nam]\n",
       "+- Relation[age#133L,name#134] json\n",
       "\u001b[39m\n",
       "  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m110\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m107\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m278\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m278\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\n",
       "  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m238\u001b[39m)\n",
       "  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\n",
       "  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\n",
       "  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\n",
       "  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m238\u001b[39m)\n",
       "  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m231\u001b[39m)\n",
       "  scala.collection.AbstractTraversable.map(\u001b[32mTraversable.scala\u001b[39m:\u001b[32m108\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m107\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m85\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m127\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m85\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m82\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m95\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m108\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m201\u001b[39m)\n",
       "  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m105\u001b[39m)\n",
       "  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m57\u001b[39m)\n",
       "  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m55\u001b[39m)\n",
       "  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m47\u001b[39m)\n",
       "  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m78\u001b[39m)\n",
       "  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3406\u001b[39m)\n",
       "  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1335\u001b[39m)\n",
       "  ammonite.$sess.cmd18$Helper.$anonfun$df$value$1(\u001b[32mcmd18.sc\u001b[39m:\u001b[32m1\u001b[39m)\n",
       "  almond.api.internal.Lazy.liftedTree1$1(\u001b[32mLazy.scala\u001b[39m:\u001b[32m10\u001b[39m)\n",
       "  almond.api.internal.Lazy.value$lzycompute(\u001b[32mLazy.scala\u001b[39m:\u001b[32m10\u001b[39m)\n",
       "  almond.api.internal.Lazy.value(\u001b[32mLazy.scala\u001b[39m:\u001b[32m8\u001b[39m)\n",
       "  ammonite.$sess.cmd19$Helper.<init>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m1\u001b[39m)\n",
       "  ammonite.$sess.cmd19$.<init>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m7\u001b[39m)\n",
       "  ammonite.$sess.cmd19$.<clinit>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lazy val df: DataFrame = spark.read.json(\"data/people.json\").select($\"nam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error will be shown at runtime: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">json at cmd18.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`nam`' given input columns: [age, name];;\n'Project ['nam]\n+- Relation[age#133L,name#134] json\n\u001b[39m\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m110\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m107\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m70\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m238\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m238\u001b[39m)\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m231\u001b[39m)\n  scala.collection.AbstractTraversable.map(\u001b[32mTraversable.scala\u001b[39m:\u001b[32m108\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m121\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m187\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m107\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m85\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m85\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m82\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m95\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m108\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m201\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m105\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m57\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m55\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m47\u001b[39m)\n  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m78\u001b[39m)\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3406\u001b[39m)\n  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1335\u001b[39m)\n  ammonite.$sess.cmd18$Helper.$anonfun$df$value$1(\u001b[32mcmd18.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  almond.api.internal.Lazy.liftedTree1$1(\u001b[32mLazy.scala\u001b[39m:\u001b[32m10\u001b[39m)\n  almond.api.internal.Lazy.value$lzycompute(\u001b[32mLazy.scala\u001b[39m:\u001b[32m10\u001b[39m)\n  almond.api.internal.Lazy.value(\u001b[32mLazy.scala\u001b[39m:\u001b[32m8\u001b[39m)\n  ammonite.$sess.cmd19$Helper.<init>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd19$.<init>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd19$.<clinit>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the contrary, the error in the dataset transformation manifests at compile-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd20.sc:1: value nam is not a member of cmd20.this.cmd12.Person\n",
      "val res20 = peopleDs.map(_.nam)\n",
      "                           ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "peopleDs.map(_.nam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `filter` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the equivalent to the typed `filter` transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd20.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|2003|Gabriel|\n",
      "|2004| Noelia|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.filter($\"age\" > 2001)\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass a column function not denoting a boolean value, we won't even get a run-time exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdf\u001b[39m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df: DataFrame = \n",
    "    people.filter($\"name\" > 2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd22.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `groupBy` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val students: DataFrame = spark.read.json(\"data/students.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.groupBy($\"degree\").count.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Join` transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already discussed joins, but we didn't mention that the resulting type of a join is a dataframe, not a dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Student(name: String, degree: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDs.join(students.as[Student], \"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problems of `Dataset`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are nice because they are type safe, but, unfortunately, they are less efficient than data frames in several respects. This can be best shown by reading from parquet source files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a _columnar_ format, which means that it stores physically data around columns, allowing us to read only data from a particular column without reading the entire row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.write.mode(\"overwrite\").parquet(\"data/people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"data/people.parquet\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `ReadSchema` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a program that simply read the _name_ column of the people dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds: Dataset[String] = \n",
    "    spark.read.parquet(\"data/people.parquet\").as[Person]\n",
    "        .map(_.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which works as intended: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem, however: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the plan includes the directive `ReadSchema: struct<age:bigint,name:string>`, which generates a query to scan the full schema of the parquet file. But we just want to read the names! We can create an optimun program using dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df: DataFrame = \n",
    "    spark.read.parquet(\"data/people.parquet\").select($\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which works similarly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but more efficiently (note the the value of the `ReadSchema` directive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can empirically check that it actually works using the Spark UI. First, we create a parquet file with enough rows and several columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(0, 1000000)\n",
    "    .select($\"id\" as \"_1\", lit(1) as \"_2\")\n",
    "    .write.mode(\"overwrite\").parquet(\"data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we read the second column using both datasets and dataframes, and check the Spark UI for the _Input Size_ field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val test = spark.read.parquet(\"data/test\")\n",
    "test.as[Tuple2[Long, Int]].map(_._2).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dataframes the input size is much lower since we only read the second column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.select($\"_2\").collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `PushedFilter` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following equivalent dataset and dataframe programs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds: Dataset[(Long, Int)] = \n",
    "    test.as[(Long, Int)]\n",
    "        .filter(_._1 >= 999995)\n",
    "\n",
    "val df: DataFrame = \n",
    "    test\n",
    "        .filter($\"_1\" >= 999995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionally, they are equivalent, but their performance differ significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect\n",
    "ds.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explanation of this difference lies in another optimization applied by the Spark SQL compiler: the so-called push-down filter optimization. In the previous `ReadSchema` optimization, we skipped certain columns of the dataset; now, we skip rows and read only the ones we are interested in (those that satisfy the predicate). We can check if the push-down filter optimization is actually applied by inspecting the query plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain\n",
    "ds.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `PartitionFilters` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a test file with an additional column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(0, 1000000)\n",
    "    .select($\"id\" as \"_1\", lit(1) as \"_2\", round(rand() * 10) mod lit(10) as \"_3\")\n",
    "    .write.mode(\"overwrite\").parquet(\"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val test: DataFrame = spark.read.parquet(\"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that we want to read data with value `_3` equal to `9.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.filter($\"_3\" === lit(9.0)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pushed filter optimization is created, but it would be better if we could just read directly those rows with the exact value for the thrid column. We can achieve that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.write.mode(\"overwrite\").partitionBy(\"_3\").parquet(\"data/testP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the parquet file is splitted into ten partitions. Now, if we just want to process data with a particular key, Spark will generate an optimun query: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testP: DataFrame = spark.read.parquet(\"data/testP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testP.filter($\"_3\" === lit(9.0)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspet the Spark UI to check that we read less data in the last action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
