{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $file.common\n",
    "import spark._\n",
    "import common._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructType}\n",
    "import org.apache.spark.sql.functions.{col, to_date}\n",
    "import spark.implicits._\n",
    "import spark.sqlContext.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query 2: Media de infecciones por Superficie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta query utiliza dos conjuntos de datos, por una parte los datos de casos reportados diariemente por país y por otro lado una tabla de datos demográficos por país en la que encontramos la superficie de cada país.\n",
    "\n",
    "El objetivo de la query es:\n",
    "    - Agrupar los datos por paises\n",
    "    - Obtener la tasa de infectados diarios por superficie (en Km^{2})\n",
    "        - Para ello dividimos los casos diarios entre la superficie\n",
    "    - Calcular la media de infecciones por superficie\n",
    "    \n",
    "De esta forma obtendremos la media de contagios por Km^{2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Utilizando RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infections(lines : RDD[String]) : RDD[Infection] =\n",
    "    lines.map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Infection(\n",
    "        day = arr(1).toInt,\n",
    "        month = arr(2).toInt,\n",
    "        year = arr(3).toInt,\n",
    "        nCases = arr(4).toInt,\n",
    "        nDeaths = arr(5).toInt,\n",
    "        country = arr(6),\n",
    "        continent = arr(10)\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionRDD = infections(spark.sparkContext.textFile(\"../datasets/data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Population(\n",
    "    country : String, \n",
    "    population : Int, \n",
    "    density : Int, \n",
    "    land_area: Int, \n",
    "    ) \n",
    "extends Serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populationData = spark.sparkContext.textFile(\"../datasets/population_by_country_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population(lines : RDD[String]) : RDD[Population] =\n",
    "    lines.mapPartitionsWithIndex(\n",
    "                   (index, it) => if (index == 0) it.drop(1) else it,\n",
    "                    preservesPartitioning = true\n",
    "                 )\n",
    "    .map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Population(\n",
    "        country = arr(0),\n",
    "        population = arr(1).toInt,\n",
    "        density = arr(4).toInt,\n",
    "        land_area = arr(5).toInt,\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populationRDD = population(populationData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 RDD Sin optimizar\n",
    "\n",
    "Aquí vemos una de las grandes desventajas de los RDDs, que al ser utilizados como colecciones y creados \"a mano\" no se aplican todas las optimizaciones que se pueden aplicar como en DF o DS gracias a Catalyst\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "Un join computacionalmente pesado desde el principio ya que cruza todos los datos sin \n",
    "quedarnos con los que nos interesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark no me deja hacer un Join de RDD que no sean pair RDD así que tenemos que construirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// populationRDD.join(infectionRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyo Pair RDDs conservando todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populationByCountry = populationRDD.map(\n",
    "    x => (x.country,x))\n",
    "\n",
    "def infectionByCountry = \n",
    "      infectionRDD.map(x => (x.country,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago el Join y agrupo por paises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinedRDD = infectionByCountry.join(populationByCountry).groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente calculo la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanRDD = joinedRDD.mapValues(\n",
    "    x => x.map( \n",
    "        line => line._1.nCases.toFloat / line._2.land_area.toFloat\n",
    "    )).mapValues(\n",
    "    x => x.sum / x.size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo hago todo en una única operación para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_RDD_Not_Optimized =\n",
    "    infectionByCountry.join(populationByCountry)\n",
    "    .groupByKey()\n",
    "    .mapValues(\n",
    "    x => x.map( \n",
    "        line => line._1.nCases.toFloat / line._2.land_area.toFloat)\n",
    "    ).mapValues(\n",
    "        x => x.sum / x.size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Para optimizar un poco este RDD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despejo solo los datos que me interesan para trabajar con Pair RDDs y optimizar la consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countriesAndLandArea = populationRDD.map(\n",
    "    x => (x.country,x.land_area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countriesAndCases = \n",
    "      infectionRDD.map(x => (x.country,x.nCases))\n",
    "      .groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuto un join y trabajo para calcular primero la media de infecciones por Km2 diaria, \n",
    "para luego calcular la media total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average = countriesAndCases.join(countriesAndLandArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg = average.mapValues(\n",
    "    x => x._1.map(\n",
    "        y => (y.toFloat / x._2.toFloat)\n",
    "    )).mapValues(\n",
    "    x => x.sum/x.size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo hago todo en una única operación para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_RDD =\n",
    "countriesAndCases.join(countriesAndLandArea)   \n",
    ".mapValues(\n",
    "    x => x._1.map(\n",
    "        y => (y.toDouble / x._2.toDouble)\n",
    "    )).mapValues(\n",
    "    x => x.sum / x.size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Consulta con DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"../datasets/covidworldwide.csv\")\n",
    ".withColumnRenamed(\"countriesAndTerritories\",\"Country\")\n",
    ".as[(String,String,String,String,Double,Double,String,String,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"../datasets/population_by_country_2020.csv\")\n",
    ".withColumnRenamed(\"Country (or dependency)\",\"Country\")\n",
    ".withColumnRenamed(\"Population (2020)\",\"Population\")\n",
    ".withColumnRenamed(\"Land Area (Km\\u00b2)\",\"Area\")\n",
    ".as[(String,Float,String,Float,Float,Float,Double,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_DS = \n",
    "infectionDS.join(populationDS, \"Country\")\n",
    "        .select($\"Country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Area\",\n",
    "                $\"cases\" / $\"Area\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"Country\")\n",
    "        .avg(\"infection Per Km\\u00b2\")\n",
    "        .orderBy(desc(\"avg(infection Per Km²)\"))\n",
    "        .as[(String,Double)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Consulta con DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfCovid = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"../datasets/covidworldwide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfPopulation = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"../datasets/population_by_country_2020.csv\")\n",
    ".withColumnRenamed(\"Country (or dependency)\",\"Country\")\n",
    ".withColumnRenamed(\"Population (2020)\",\"Population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifico los datos de entrada para que el formato fecha se adecue al TimeStamp de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfCovidClean = dfCovid\n",
    "    .select($\"*\",$\"dateRep\",translate($\"dateRep\",\"/\",\"-\").as(\"date\"))\n",
    "    .drop(\"dateRep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfCovidDate = dfCovidClean\n",
    "    .select($\"*\",col(\"date\"),to_date(col(\"date\"),\"dd-MM-yyyy\").as(\"to_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago una consulta de prueba para obtener la media solo de los casos en España"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spainCovid = dfCovid\n",
    "    .select(\"dateRep\",\"cases\")\n",
    "    .where(\"countriesAndTerritories == 'Spain'\").toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente ejecuto la consulta de nuestro caso de uso, infecciones por Km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_DF = \n",
    "dfCovid.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "        .select($\"country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"country\")\n",
    "        .avg(\"infection Per Km\\u00b2\")\n",
    "        .orderBy(desc(\"avg(infection Per Km²)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualización de rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val (x, y) = Seq(\n",
    "    \"Not Optimized RDD\" -> runWithOutput(query_RDD_Not_Optimized.collect()),\n",
    "    \"RDD\" -> runWithOutput(query_RDD.collect()),\n",
    "    \"DataSet\" -> runWithOutput(query_DS.collect),\n",
    "    \"DataFrame\" -> runWithOutput(query_DF.collect)\n",
    ").unzip\n",
    "\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Comparativas de rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time(query_RDD_Not_Optimized.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time(query_RDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time(query_DS.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time(query_DF.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(query_RDD_Not_Optimized.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(query_RDD.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(query_DS.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    query_DF.collect\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualización de datos con plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x,y) = query_DF.collect.map(r=>(r(0).toString, r(1).toString.toFloat)).toList.unzip\n",
    "Bar(x, y).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
