{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /home/jovyan/work/notebooks/common.scLoading spark-stubs\n",
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/06/16 14:08:15 INFO SparkContext: Running Spark version 2.4.5\n",
      "21/06/16 14:08:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/06/16 14:08:15 INFO SparkContext: Submitted application: Queries Optimization\n",
      "21/06/16 14:08:15 INFO SecurityManager: Changing view acls to: jovyan\n",
      "21/06/16 14:08:15 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "21/06/16 14:08:15 INFO SecurityManager: Changing view acls groups to: \n",
      "21/06/16 14:08:15 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/06/16 14:08:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "21/06/16 14:08:15 INFO Utils: Successfully started service 'sparkDriver' on port 34373.\n",
      "21/06/16 14:08:16 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/06/16 14:08:16 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/06/16 14:08:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/06/16 14:08:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/06/16 14:08:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-29445cb5-d882-4b5a-ad50-0689e855a3e5\n",
      "21/06/16 14:08:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "21/06/16 14:08:16 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/06/16 14:08:16 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "21/06/16 14:08:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://889bd5fba3d9:4042\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0-sources.jar at spark://889bd5fba3d9:34373/jars/jvm-repr-0.4.0-sources.jar with timestamp 1623852496405\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0.jar at spark://889bd5fba3d9:34373/jars/jvm-repr-0.4.0.jar with timestamp 1623852496406\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1-sources.jar at spark://889bd5fba3d9:34373/jars/scopt_2.12-3.7.1-sources.jar with timestamp 1623852496407\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1.jar at spark://889bd5fba3d9:34373/jars/scopt_2.12-3.7.1.jar with timestamp 1623852496407\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.10/1.7.4/ammonite-interp-api_2.12.10-1.7.4-sources.jar at spark://889bd5fba3d9:34373/jars/ammonite-interp-api_2.12.10-1.7.4-sources.jar with timestamp 1623852496407\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.10/1.7.4/ammonite-interp-api_2.12.10-1.7.4.jar at spark://889bd5fba3d9:34373/jars/ammonite-interp-api_2.12.10-1.7.4.jar with timestamp 1623852496407\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/1.7.4/ammonite-ops_2.12-1.7.4-sources.jar at spark://889bd5fba3d9:34373/jars/ammonite-ops_2.12-1.7.4-sources.jar with timestamp 1623852496408\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/1.7.4/ammonite-ops_2.12-1.7.4.jar at spark://889bd5fba3d9:34373/jars/ammonite-ops_2.12-1.7.4.jar with timestamp 1623852496408\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.10/1.7.4/ammonite-repl-api_2.12.10-1.7.4-sources.jar at spark://889bd5fba3d9:34373/jars/ammonite-repl-api_2.12.10-1.7.4-sources.jar with timestamp 1623852496409\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.10/1.7.4/ammonite-repl-api_2.12.10-1.7.4.jar at spark://889bd5fba3d9:34373/jars/ammonite-repl-api_2.12.10-1.7.4.jar with timestamp 1623852496409\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/1.7.4/ammonite-util_2.12-1.7.4-sources.jar at spark://889bd5fba3d9:34373/jars/ammonite-util_2.12-1.7.4-sources.jar with timestamp 1623852496409\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/1.7.4/ammonite-util_2.12-1.7.4.jar at spark://889bd5fba3d9:34373/jars/ammonite-util_2.12-1.7.4.jar with timestamp 1623852496410\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.7/fansi_2.12-0.2.7-sources.jar at spark://889bd5fba3d9:34373/jars/fansi_2.12-0.2.7-sources.jar with timestamp 1623852496412\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.7/fansi_2.12-0.2.7.jar at spark://889bd5fba3d9:34373/jars/fansi_2.12-0.2.7.jar with timestamp 1623852496415\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.1.8/geny_2.12-0.1.8-sources.jar at spark://889bd5fba3d9:34373/jars/geny_2.12-0.1.8-sources.jar with timestamp 1623852496416\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.1.8/geny_2.12-0.1.8.jar at spark://889bd5fba3d9:34373/jars/geny_2.12-0.1.8.jar with timestamp 1623852496416\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.3.0/os-lib_2.12-0.3.0-sources.jar at spark://889bd5fba3d9:34373/jars/os-lib_2.12-0.3.0-sources.jar with timestamp 1623852496417\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.3.0/os-lib_2.12-0.3.0.jar at spark://889bd5fba3d9:34373/jars/os-lib_2.12-0.3.0.jar with timestamp 1623852496417\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.5/pprint_2.12-0.5.5-sources.jar at spark://889bd5fba3d9:34373/jars/pprint_2.12-0.5.5-sources.jar with timestamp 1623852496418\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.5/pprint_2.12-0.5.5.jar at spark://889bd5fba3d9:34373/jars/pprint_2.12-0.5.5.jar with timestamp 1623852496418\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.1.7/sourcecode_2.12-0.1.7-sources.jar at spark://889bd5fba3d9:34373/jars/sourcecode_2.12-0.1.7-sources.jar with timestamp 1623852496418\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.1.7/sourcecode_2.12-0.1.7.jar at spark://889bd5fba3d9:34373/jars/sourcecode_2.12-0.1.7.jar with timestamp 1623852496418\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/0.0.12/interface-0.0.12-sources.jar at spark://889bd5fba3d9:34373/jars/interface-0.0.12-sources.jar with timestamp 1623852496419\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/0.0.12/interface-0.0.12.jar at spark://889bd5fba3d9:34373/jars/interface-0.0.12.jar with timestamp 1623852496419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.0.0/scala-collection-compat_2.12-2.0.0-sources.jar at spark://889bd5fba3d9:34373/jars/scala-collection-compat_2.12-2.0.0-sources.jar with timestamp 1623852496420\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.0.0/scala-collection-compat_2.12-2.0.0.jar at spark://889bd5fba3d9:34373/jars/scala-collection-compat_2.12-2.0.0.jar with timestamp 1623852496420\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.2.0/scala-xml_2.12-1.2.0-sources.jar at spark://889bd5fba3d9:34373/jars/scala-xml_2.12-1.2.0-sources.jar with timestamp 1623852496420\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.2.0/scala-xml_2.12-1.2.0.jar at spark://889bd5fba3d9:34373/jars/scala-xml_2.12-1.2.0.jar with timestamp 1623852496420\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.10/scala-compiler-2.12.10-sources.jar at spark://889bd5fba3d9:34373/jars/scala-compiler-2.12.10-sources.jar with timestamp 1623852496421\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.10/scala-library-2.12.10-sources.jar at spark://889bd5fba3d9:34373/jars/scala-library-2.12.10-sources.jar with timestamp 1623852496421\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.10/scala-reflect-2.12.10-sources.jar at spark://889bd5fba3d9:34373/jars/scala-reflect-2.12.10-sources.jar with timestamp 1623852496422\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.9.1/interpreter-api_2.12-0.9.1-sources.jar at spark://889bd5fba3d9:34373/jars/interpreter-api_2.12-0.9.1-sources.jar with timestamp 1623852496422\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.9.1/interpreter-api_2.12-0.9.1.jar at spark://889bd5fba3d9:34373/jars/interpreter-api_2.12-0.9.1.jar with timestamp 1623852496422\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.9.1/jupyter-api_2.12-0.9.1-sources.jar at spark://889bd5fba3d9:34373/jars/jupyter-api_2.12-0.9.1-sources.jar with timestamp 1623852496422\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.9.1/jupyter-api_2.12-0.9.1.jar at spark://889bd5fba3d9:34373/jars/jupyter-api_2.12-0.9.1.jar with timestamp 1623852496423\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.10/0.9.1/scala-kernel-api_2.12.10-0.9.1-sources.jar at spark://889bd5fba3d9:34373/jars/scala-kernel-api_2.12.10-0.9.1-sources.jar with timestamp 1623852496423\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.10/0.9.1/scala-kernel-api_2.12.10-0.9.1.jar at spark://889bd5fba3d9:34373/jars/scala-kernel-api_2.12.10-0.9.1.jar with timestamp 1623852496423\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.local/share/jupyter/kernels/scala212/launcher.jar at spark://889bd5fba3d9:34373/jars/launcher.jar with timestamp 1623852496424\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.1/scala-reflect-2.12.1.jar at spark://889bd5fba3d9:34373/jars/scala-reflect-2.12.1.jar with timestamp 1623852496424\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/plotly-scala/plotly-core_2.12/0.8.1/plotly-core_2.12-0.8.1.jar at spark://889bd5fba3d9:34373/jars/plotly-core_2.12-0.8.1.jar with timestamp 1623852496424\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/plotly-scala/plotly-render_2.12/0.8.1/plotly-render_2.12-0.8.1.jar at spark://889bd5fba3d9:34373/jars/plotly-render_2.12-0.8.1.jar with timestamp 1623852496424\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/plotly-scala/plotly-almond_2.12/0.8.1/plotly-almond_2.12-0.8.1.jar at spark://889bd5fba3d9:34373/jars/plotly-almond_2.12-0.8.1.jar with timestamp 1623852496424\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/webjars/bower/plotly.js/1.52.2/plotly.js-1.52.2.jar at spark://889bd5fba3d9:34373/jars/plotly.js-1.52.2.jar with timestamp 1623852496424\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.11/scala-library-2.12.11.jar at spark://889bd5fba3d9:34373/jars/scala-library-2.12.11.jar with timestamp 1623852496424\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar at spark://889bd5fba3d9:34373/jars/scala-reflect-2.12.11.jar with timestamp 1623852496425\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.9.9/jackson-annotations-2.9.9.jar at spark://889bd5fba3d9:34373/jars/jackson-annotations-2.9.9.jar with timestamp 1623852496425\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/moshi/moshi/1.5.0/moshi-1.5.0.jar at spark://889bd5fba3d9:34373/jars/moshi-1.5.0.jar with timestamp 1623852496426\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/retrofit2/converter-moshi/2.4.0/converter-moshi-2.4.0.jar at spark://889bd5fba3d9:34373/jars/converter-moshi-2.4.0.jar with timestamp 1623852496426\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/ch/cern/sparkmeasure/spark-measure_2.12/0.17/spark-measure_2.12-0.17.jar at spark://889bd5fba3d9:34373/jars/spark-measure_2.12-0.17.jar with timestamp 1623852496426\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/msgpack/msgpack-core/0.8.16/msgpack-core-0.8.16.jar at spark://889bd5fba3d9:34373/jars/msgpack-core-0.8.16.jar with timestamp 1623852496427\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.9.9/jackson-databind-2.9.9.jar at spark://889bd5fba3d9:34373/jars/jackson-databind-2.9.9.jar with timestamp 1623852496427\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar at spark://889bd5fba3d9:34373/jars/slf4j-api-1.7.26.jar with timestamp 1623852496428\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/org/influxdb/influxdb-java/2.14/influxdb-java-2.14.jar at spark://889bd5fba3d9:34373/jars/influxdb-java-2.14.jar with timestamp 1623852496428\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/okhttp3/okhttp/3.11.0/okhttp-3.11.0.jar at spark://889bd5fba3d9:34373/jars/okhttp-3.11.0.jar with timestamp 1623852496428\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/retrofit2/retrofit/2.4.0/retrofit-2.4.0.jar at spark://889bd5fba3d9:34373/jars/retrofit-2.4.0.jar with timestamp 1623852496429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.9.9/jackson-module-scala_2.12-2.9.9.jar at spark://889bd5fba3d9:34373/jars/jackson-module-scala_2.12-2.9.9.jar with timestamp 1623852496429\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-paranamer/2.9.9/jackson-module-paranamer-2.9.9.jar at spark://889bd5fba3d9:34373/jars/jackson-module-paranamer-2.9.9.jar with timestamp 1623852496429\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/okhttp3/logging-interceptor/3.11.0/logging-interceptor-3.11.0.jar at spark://889bd5fba3d9:34373/jars/logging-interceptor-3.11.0.jar with timestamp 1623852496429\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/okio/okio/1.14.0/okio-1.14.0.jar at spark://889bd5fba3d9:34373/jars/okio-1.14.0.jar with timestamp 1623852496430\n",
      "21/06/16 14:08:16 INFO SparkContext: Added JAR file:/home/jovyan/.cache/coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.9.9/jackson-core-2.9.9.jar at spark://889bd5fba3d9:34373/jars/jackson-core-2.9.9.jar with timestamp 1623852496430\n",
      "21/06/16 14:08:16 INFO Executor: Starting executor ID driver on host localhost\n",
      "21/06/16 14:08:16 INFO Executor: Using REPL class URI: http://172.17.0.2:39803\n",
      "21/06/16 14:08:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42893.\n",
      "21/06/16 14:08:16 INFO NettyBlockTransferService: Server created on 889bd5fba3d9:42893\n",
      "21/06/16 14:08:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/06/16 14:08:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 889bd5fba3d9, 42893, None)\n",
      "21/06/16 14:08:16 INFO BlockManagerMasterEndpoint: Registering block manager 889bd5fba3d9:42893 with 366.3 MB RAM, BlockManagerId(driver, 889bd5fba3d9, 42893, None)\n",
      "21/06/16 14:08:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 889bd5fba3d9, 42893, None)\n",
      "21/06/16 14:08:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 889bd5fba3d9, 42893, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://889bd5fba3d9:4042\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd0.sc:6: not found: value func\n",
      "import org.apache.spark.sql.types._, func._\n",
      "                                     ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "import $file.common\n",
    "import spark._\n",
    "import common._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructType}\n",
    "import org.apache.spark.sql.types._, func._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Media de infecciones por Km2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infections(lines : RDD[String]) : RDD[Infection] =\n",
    "    lines.map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Infection(\n",
    "        day = arr(1).toInt,\n",
    "        month = arr(2).toInt,\n",
    "        year = arr(3).toInt,\n",
    "        nCases = arr(4).toInt,\n",
    "        nDeaths = arr(5).toInt,\n",
    "        country = arr(6),\n",
    "        continent = arr(10)\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionRDD = infections(spark.sparkContext.textFile(\"../datasets/data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Population(\n",
    "    country : String, \n",
    "    population : Int, \n",
    "    density : Int, \n",
    "    land_area: Int, \n",
    "    ) \n",
    "extends Serializable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationData = spark.sparkContext.textFile(\"../datasets/population_by_country_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val header = populationData.first() \n",
    "\n",
    "def population(lines : RDD[String]) : RDD[Population] =\n",
    "    lines.filter(x => x != header)\n",
    "    .map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Population(\n",
    "        country = arr(0),\n",
    "        population = arr(1).toInt,\n",
    "        density = arr(4).toInt,\n",
    "        land_area = arr(5).toInt,\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationRDD = population(populationData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un join computacionalmente pesado desde el principio ya que cruza todos los datos sin quedarnos con los que nos interesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark no me deja hacer un Join de RDD que no sean pair RDD así que tenemos que construirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// populationRDD.join(infectionRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyo Pair RDDs conservando todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationByCountry = populationRDD.map(\n",
    "    x => (x.country,x))\n",
    "\n",
    "val infectionByCountry = \n",
    "      infectionRDD.map(x => (x.country,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago el Join y agrupo por paises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinedRDD = infectionByCountry.join(populationByCountry).groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente calculo la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedRDD.mapValues(\n",
    "    x => x.map( \n",
    "        line => line._1.nCases.toFloat / line._2.land_area.toFloat\n",
    "    )).mapValues(\n",
    "    x => x.sum / x.size\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo hago todo en una única operación para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val notOptimizedRDD =\n",
    "    infectionByCountry.join(populationByCountry)\n",
    "    .groupByKey()\n",
    "    .mapValues(\n",
    "    x => x.map( \n",
    "        line => line._1.nCases.toFloat / line._2.land_area.toFloat)\n",
    "    ).mapValues(\n",
    "        x => x.sum / x.size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para optimizar un poco esta consulta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despejo solo los datos que me interesan para trabajar con Pair RDDs y optimizar la consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val countriesAndLandArea = populationRDD.map(\n",
    "    x => (x.country,x.land_area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val countriesAndCases = \n",
    "      infectionRDD.map(x => (x.country,x.nCases))\n",
    "      .groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuto un join y trabajo para calcular primero la media de infecciones por Km2 diaria, \n",
    "para luego calcular la media total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val average = countriesAndCases.join(countriesAndLandArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average.mapValues(\n",
    "    x => x._1.map(\n",
    "        y => (y.toFloat / x._2.toFloat)\n",
    "    )).mapValues(\n",
    "    x => x.sum/x.size\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo hago todo en una única operación para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val meanInfectionsRDD =\n",
    "countriesAndCases.join(countriesAndLandArea)   \n",
    ".mapValues(\n",
    "    x => x._1.map(\n",
    "        y => (y.toDouble / x._2.toDouble)\n",
    "    )).mapValues(\n",
    "    x => x.sum / x.size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    meanInfectionsRDD.collect\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta con DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"../datasets/covidworldwide.csv\")\n",
    ".withColumnRenamed(\"countriesAndTerritories\",\"Country\")\n",
    ".as[(String,String,String,String,Double,Double,String,String,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"population_by_country_2020.csv\")\n",
    ".withColumnRenamed(\"Country (or dependency)\",\"Country\")\n",
    ".withColumnRenamed(\"Population (2020)\",\"Population\")\n",
    ".as[(String,Float,String,Float,Float,Float,Double,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val meanInfectionsperKM2DS = \n",
    "infectionDS.join(populationDS, \"Country\")\n",
    "        .select($\"Country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"Country\")\n",
    "        .agg(round(avg(\"infection Per Km\\u00b2\"),10).as[Float])\n",
    "        .orderBy(desc(\"round(avg(infection Per Km²), 10)\"))\n",
    "        .as[(String,Double)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(meanInfectionsperKM2DS.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta con DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovid = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"covidworldwide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfPopulation = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"population_by_country_2020.csv\")\n",
    ".withColumnRenamed(\"Country (or dependency)\",\"Country\")\n",
    ".withColumnRenamed(\"Population (2020)\",\"Population\")\n",
    "dfPopulation.showHTML()\n",
    "dfPopulation.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifico los datos de entrada para que el formato fecha se adecue al TimeStamp de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovidClean = dfCovid\n",
    "    .select($\"*\",$\"dateRep\",translate($\"dateRep\",\"/\",\"-\").as(\"date\"))\n",
    "    .drop(\"dateRep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovidDate = dfCovidClean\n",
    "    .select($\"*\",col(\"date\"),to_date(col(\"date\"),\"dd-MM-yyyy\").as(\"to_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago una consulta de prueba para obtener la media solo de los casos en España"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spainCovid = dfCovid.select(\"dateRep\",\"cases\").where(\"countriesAndTerritories == 'Spain'\").toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spainCovid.agg(avg(\"cases\")).showHTML()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cruzo los datos con un Join y hago algunas consultas sencillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val megaDF = dfCovid.join(dfMeasures, $\"Country\" === $\"countriesAndTerritories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    \n",
    "    dfCovid.join(dfMeasures, $\"Country\" === $\"countriesAndTerritories\")\n",
    "        .select(\"cases\",\"deaths\",\"dateRep\",\"Response_measure\")\n",
    "        .where(\"countriesAndTerritories == 'Spain'\")\n",
    "        .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente ejecuto la consulta de nuestro caso de uso, infecciones por Km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val meanInfectionsperKM2DF = \n",
    "dfCovid.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "        .select($\"country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"country\")\n",
    "        .avg(\"infection Per Km\\u00b2\")\n",
    "        .orderBy(desc(\"avg(infection Per Km²)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    meanInfectionsperKM2DF.collect\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de datos con plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x,y) = meanInfectionsperKM2DF.collect.map(r=>(r(0).toString, r(1).toString.toFloat)).toList.unzip\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de eficiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x, y) = Seq(\n",
    "    \"Not Optimized RDD\" -> runWithOutput(notOptimizedRDD.collect),\n",
    "    \"RDD\" -> runWithOutput(meanInfectionsRDD.collect),\n",
    "    \"DataSet\" -> runWithOutput(meanInfectionsperKM2DS.collect),\n",
    "    \"DataFrame\" -> runWithOutput(meanInfectionsperKM2DF.collect),\n",
    ").unzip\n",
    "\n",
    "Bar(x, y).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
