{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de Consultas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creamos la sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "\n",
    "import org.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
    "\n",
    "val spark: SparkSession = \n",
    "    NotebookSparkSession\n",
    "      .builder()\n",
    "      .appName(\"Queries Optimization\")\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.8.1`\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`ch.cern.sparkmeasure:spark-measure_2.12:0.17`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "import spark.sqlContext.implicits._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructType}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.types._, func._\n",
    "import org.apache.spark.sql.functions.{col, to_date}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset ha sido obtenido de:\n",
    "https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide\n",
    "\n",
    "En el se observan los casos diarios de Covid-19 por país hasta el 14-12-20\n",
    "\n",
    "En la segunda parte se utilizan los datos de las medidas aplicadas a cada país por fecha de inicio y fin:\n",
    "\n",
    "https://www.ecdc.europa.eu/en/publications-data/download-data-response-measures-covid-19\n",
    "\n",
    "La consulta para calcular las infecciones por km2:\n",
    "\n",
    "https://www.kaggle.com/tanuprabhu/population-by-country-2020\n",
    "\n",
    "Y por último trabajaremos también con vacunaciones:\n",
    "\n",
    "https://www.kaggle.com/gpreda/covid-world-vaccination-progress\n",
    "\n",
    "tendremos que utilizar otro dataset con datos de covid ya que en el original no vienen todas las fechas:\n",
    "\n",
    "https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo una clase para trabajar con infecciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Infection(day : Int, \n",
    "                     month : Int, \n",
    "                     year : Int, \n",
    "                     nCases: Int, \n",
    "                     nDeaths : Int, \n",
    "                     country : String,  \n",
    "                     continent : String) \n",
    "extends Serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y un método para medir tiempos de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runWithOutput[A](code: => A): Int = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    val out = System.currentTimeMillis() - start\n",
    "    println(s\"Took ${System.currentTimeMillis() - start}\")\n",
    "    out.toInt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para utilizar showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Credit to Aivean\n",
    "implicit class RichDF(val ds:DataFrame) {\n",
    "    def showHTML(limit:Int = 20, truncate: Int = 20) = {\n",
    "        import xml.Utility.escape\n",
    "        val data = ds.take(limit)\n",
    "        val header = ds.schema.fieldNames.toSeq        \n",
    "        val rows: Seq[Seq[String]] = data.map { row =>\n",
    "          row.toSeq.map { cell =>\n",
    "            val str = cell match {\n",
    "              case null => \"null\"\n",
    "              case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\")\n",
    "              case array: Array[_] => array.mkString(\"[\", \", \", \"]\")\n",
    "              case seq: Seq[_] => seq.mkString(\"[\", \", \", \"]\")\n",
    "              case _ => cell.toString\n",
    "            }\n",
    "            if (truncate > 0 && str.length > truncate) {\n",
    "              // do not show ellipses for strings shorter than 4 characters.\n",
    "              if (truncate < 4) str.substring(0, truncate)\n",
    "              else str.substring(0, truncate - 3) + \"...\"\n",
    "            } else {\n",
    "              str\n",
    "            }\n",
    "          }: Seq[String]\n",
    "        }\n",
    "publish.html(s\"\"\" <table>\n",
    "                <tr>\n",
    "                 ${header.map(h => s\"<th>${escape(h)}</th>\").mkString}\n",
    "                </tr>\n",
    "                ${rows.map { row =>\n",
    "                  s\"<tr>${row.map{c => s\"<td>${escape(c)}</td>\" }.mkString}</tr>\"\n",
    "                }.mkString}\n",
    "            </table>\n",
    "        \"\"\")        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empiezo trabajando con RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionData = spark.sparkContext.textFile(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una funcion para trabajar con un RDD de infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infections(lines : RDD[String]) : RDD[Infection] =\n",
    "    lines.map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Infection(\n",
    "        day = arr(1).toInt,\n",
    "        month = arr(2).toInt,\n",
    "        year = arr(3).toInt,\n",
    "        nCases = arr(4).toInt,\n",
    "        nDeaths = arr(5).toInt,\n",
    "        country = arr(6),\n",
    "        continent = arr(10)\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo la media de infecciones diarias por país trabajando con pair RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def infectionGrowthAverage(infections : RDD[Infection]) : RDD[(String, Int)]= {\n",
    "\n",
    "    val countriesAndCases : RDD[(String, Iterable[Int])] = \n",
    "      infections.map(x => (x.country,x.nCases))\n",
    "      .groupByKey()\n",
    "      \n",
    "    countriesAndCases.mapValues(x => (x.sum / x.size)).sortBy(_._2)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestro el resultado y el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionRDD = infections(infectionData)\n",
    "val infectionAvgRDD = infectionGrowthAverage(infectionRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la API de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val timeRDD = spark.time(infectionAvgRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o bien el framework del cern que nos da más información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(infectionAvgRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hago los mismos calculos con un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierto el RDD obtenido previamente en un DataFrame para inferir la clase infección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDF = spark.createDataFrame(infectionRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizo los métodos de la clase DF que incluye uno optimizado para calcular la media.\n",
    "\n",
    "Ejecuto y comprabamos como el tiempo de ejecución es significativamente menor que en RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infAvgOrDf = infectionDF.\n",
    "    groupBy(\"country\")\n",
    "    .avg(\"nCases\")\n",
    "    .orderBy(desc(\"avg(nCases)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "infAvgOrDf.showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time(infAvgOrDf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val timeDF = spark.time(infAvgOrDf.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(infAvgOrDf.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es crear el DataFrame directamente importando los datos pero deja de ser un DF de infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val dfCovid = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"covidworldwide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovidWithSchema = dfCovid.toDF\n",
    "    .groupBy(\"countriesAndTerritories\")\n",
    "    .agg(mean(\"cases\"))\n",
    "    .orderBy(\"avg(cases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(dfCovidWithSchema.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "puedo definir el esquema manualmente para crear el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Defino el esquema manualmente pero podría verlo importando el csv y viendo como lo hace de base spark\n",
    "\n",
    "val schema = new StructType()\n",
    "    .add(\"dateRep\",StringType,true)\n",
    "    .add(\"day\",IntegerType,true)\n",
    "    .add(\"month\",IntegerType,true)\n",
    "    .add(\"year\",IntegerType,true)\n",
    "    .add(\"cases\",IntegerType,true)\n",
    "    .add(\"deaths\",IntegerType,true)\n",
    "    .add(\"countriesAndTerritories\",StringType,true)\n",
    "    .add(\"geoId\",StringType,true)\n",
    "    .add(\"countryterritoryCode\",StringType,true)\n",
    "    .add(\"popData2018\",IntegerType,true)\n",
    "    .add(\"continentExp\",StringType,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read\n",
    ".format(\"csv\")\n",
    ".option(\"header\",\"true\")\n",
    ".schema(schema)\n",
    ".load(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y con un DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".csv(\"covidworldwide.csv\")\n",
    ".as[(String,String,String,String,String,String,String,String,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val avgDS = \n",
    "    infectionDS.groupBy($\"countriesAndTerritories\")\n",
    "    .agg(avg($\"cases\"))\n",
    "    .orderBy(\"avg(cases)\")\n",
    "    .as[(String,Double)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(avgDS.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajamos con Dataset[Infection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDataset = spark.createDataset(infectionRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val avgInfectionDS = infectionDataset\n",
    "    .groupBy($\"country\")\n",
    "    .agg(avg($\"nCases\").as[Double])\n",
    "    .orderBy(\"avg(nCases)\")\n",
    "    .as[(String,Double)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(avgInfectionDS.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizo una segunda tabla y cruzo datos con RDD, DS y DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo una consulta para calcular la media de infecciones por Km2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationData = spark.sparkContext.textFile(\"population_by_country_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Population(\n",
    "    country : String, \n",
    "    population : Int, \n",
    "    density : Int, \n",
    "    land_area: Int, \n",
    "    ) \n",
    "extends Serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpio la primera linea del CSV y creo un RDD de población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val header = populationData.first() \n",
    "\n",
    "def population(lines : RDD[String]) : RDD[Population] =\n",
    "    lines.filter(x => x != header)\n",
    "    .map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Population(\n",
    "        country = arr(0),\n",
    "        population = arr(1).toInt,\n",
    "        density = arr(4).toInt,\n",
    "        land_area = arr(5).toInt,\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compruebo que se visualizan correctamente los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationRDD = population(populationData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populationRDD.toDF.showHTML()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un join computacionalmente pesado desde el principio ya que cruza todos los datos sin quedarnos con los que nos interesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark no me deja hacer un Join de RDD que no sean pair RDD así que tenemos que construirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// populationRDD.join(infectionRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyo Pair RDDs conservando todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationByCountry = populationRDD.map(\n",
    "    x => (x.country,x))\n",
    "\n",
    "val infectionByCountry = \n",
    "      infectionRDD.map(x => (x.country,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago el Join y agrupo por paises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val megaRDD = infectionByCountry.join(populationByCountry).groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente calculo la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megaRDD.mapValues(\n",
    "    x => x.map( \n",
    "        line => line._1.nCases.toFloat / line._2.land_area.toFloat\n",
    "    )).mapValues(\n",
    "    x => x.sum / x.size\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo hago todo en una única operación para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val notOptimizedRDD =\n",
    "    infectionByCountry.join(populationByCountry)\n",
    "    .groupByKey()\n",
    "    .mapValues(\n",
    "    x => x.map( \n",
    "        line => line._1.nCases.toFloat / line._2.land_area.toFloat)\n",
    "    ).mapValues(\n",
    "        x => x.sum / x.size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Hay alguna diferencia cruzando los datos en orden inverso? Parece que no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    populationByCountry.join(infectionByCountry)\n",
    "    .groupByKey()\n",
    "    .mapValues(\n",
    "    x => x.map( \n",
    "        line => line._1.land_area.toFloat / line._2.nCases.toFloat)\n",
    "    ).mapValues(\n",
    "        x => x.sum / x.size\n",
    "    ).collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para optimizar un poco esta consulta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despejo solo los datos que me interesan para trabajar con Pair RDDs y optimizar la consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val countriesAndLandArea = populationRDD.map(\n",
    "    x => (x.country,x.land_area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val countriesAndCases = \n",
    "      infectionRDD.map(x => (x.country,x.nCases))\n",
    "      .groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuto un join y trabajo para calcular primero la media de infecciones por Km2 diaria, \n",
    "para luego calcular la media total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val average = countriesAndCases.join(countriesAndLandArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average.mapValues(\n",
    "    x => x._1.map(\n",
    "        y => (y.toFloat / x._2.toFloat)\n",
    "    )).mapValues(\n",
    "    x => x.sum/x.size\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo hago todo en una única operación para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val meanInfectionsRDD =\n",
    "countriesAndCases.join(countriesAndLandArea)   \n",
    ".mapValues(\n",
    "    x => x._1.map(\n",
    "        y => (y.toDouble / x._2.toDouble)\n",
    "    )).mapValues(\n",
    "    x => x.sum / x.size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    meanInfectionsRDD.collect\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta con DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"covidworldwide.csv\")\n",
    ".withColumnRenamed(\"countriesAndTerritories\",\"Country\")\n",
    ".as[(String,String,String,String,Double,Double,String,String,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"population_by_country_2020.csv\")\n",
    ".withColumnRenamed(\"Country (or dependency)\",\"Country\")\n",
    ".withColumnRenamed(\"Population (2020)\",\"Population\")\n",
    ".as[(String,Float,String,Float,Float,Float,Double,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### infecciones por Km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val meanInfectionsperKM2DS = \n",
    "infectionDS.join(populationDS, \"Country\")\n",
    "        .select($\"Country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"Country\")\n",
    "        .agg(round(avg(\"infection Per Km\\u00b2\"),10).as[Float])\n",
    "        .orderBy(desc(\"round(avg(infection Per Km²), 10)\"))\n",
    "        .as[(String,Double)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(meanInfectionsperKM2DS.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### infecciones por número de habitantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val meanInfectionPerPopulationDS = \n",
    "infectionDS.join(populationDS, \"Country\")\n",
    "        .select($\"Country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "               $\"cases\" / $\"Population\" as \"infection Per Population\")\n",
    "        .groupBy(\"country\")\n",
    "        .avg(\"infection Per Population\")\n",
    "        .orderBy(desc(\"avg(infection Per Population)\"))\n",
    "        .as[(String,Double)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(meanInfectionPerPopulationDS.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta con DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovid = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"covidworldwide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfMeasures = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"response_graphs_data_2021-04-15.csv\")\n",
    "dfMeasures.show\n",
    "dfMeasures.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfPopulation = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"population_by_country_2020.csv\")\n",
    ".withColumnRenamed(\"Country (or dependency)\",\"Country\")\n",
    ".withColumnRenamed(\"Population (2020)\",\"Population\")\n",
    "dfPopulation.showHTML()\n",
    "dfPopulation.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifico los datos de entrada para que el formato fecha se adecue al TimeStamp de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovidClean = dfCovid\n",
    "    .select($\"*\",$\"dateRep\",translate($\"dateRep\",\"/\",\"-\").as(\"date\"))\n",
    "    .drop(\"dateRep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovidDate = dfCovidClean\n",
    "    .select($\"*\",col(\"date\"),to_date(col(\"date\"),\"dd-MM-yyyy\").as(\"to_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago una consulta de prueba para obtener la media solo de los casos en España"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spainCovid = dfCovid.select(\"dateRep\",\"cases\").where(\"countriesAndTerritories == 'Spain'\").toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spainCovid.agg(avg(\"cases\")).showHTML()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cruzo los datos con un Join y hago algunas consultas sencillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val megaDF = dfCovid.join(dfMeasures, $\"Country\" === $\"countriesAndTerritories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    \n",
    "    dfCovid.join(dfMeasures, $\"Country\" === $\"countriesAndTerritories\")\n",
    "        .select(\"cases\",\"deaths\",\"dateRep\",\"Response_measure\")\n",
    "        .where(\"countriesAndTerritories == 'Spain'\")\n",
    "        .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente ejecuto la consulta de nuestro caso de uso, infecciones por Km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val meanInfectionsperKM2DF = \n",
    "dfCovid.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "        .select($\"country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"country\")\n",
    "        .avg(\"infection Per Km\\u00b2\")\n",
    "        .orderBy(desc(\"avg(infection Per Km²)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    meanInfectionsperKM2DF.collect\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media de casos por número de habitante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionsPerPopulation = dfCovid.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "        .select($\"country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Population\",\n",
    "                $\"cases\" / $\"Population\" as \"infection Per Population\")\n",
    "        .groupBy(\"country\")\n",
    "        .avg(\"infection Per Population\")\n",
    "        .orderBy(desc(\"avg(infection Per Population)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porcentaje diario de infectados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val diaryInfectionsDF =\n",
    "dfCovidDate.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "        .select($\"country\",\n",
    "                $\"to_date\",\n",
    "                $\"day\",\n",
    "                $\"month\",\n",
    "                $\"cases\",\n",
    "                $\"Population\",\n",
    "                $\"cases\" / $\"Population\" as \"infection Per Population\")\n",
    "        .orderBy($\"to_date\".asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    diaryInfectionsDF.collect\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta con vacunaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa de infecciones frente a vacunaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd0.sc:1: not found: value spark\n",
      "val dfCovid2 = spark.read\n",
      "               ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "val dfCovid2 = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"covid_19_data.csv\")\n",
    "dfCovid2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val vaccinations = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"country_vaccinations.csv\")\n",
    "vaccinations.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modifico los datos de entrada para que se ajuste la fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val vaccinationsClean = vaccinations\n",
    "    .select($\"*\",col(\"date\"),to_date(col(\"date\"),\"MM-dd-yyyy\")\n",
    "            .as(\"dateVaccinated\"))\n",
    "    .drop(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovidClean2 = dfCovid2\n",
    "    .select($\"*\",$\"ObservationDate\",translate($\"ObservationDate\",\"/\",\"-\")\n",
    "            .as(\"date1\"))\n",
    "    .drop(\"ObservationDate\")\n",
    "    .select($\"*\",col(\"date1\"),to_date(col(\"date1\"),\"MM-dd-yyyy\")\n",
    "            .as(\"date\"))\n",
    "    .drop(\"date1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "triple join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovidClean2.join(\n",
    "    vaccinationsClean,$\"date\" === $\"dateVaccinated\"\n",
    "    && dfCovidClean2(\"Country/Region\") <=> vaccinationsClean(\"country\")\n",
    ").join(dfPopulation, \"country\").showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val megaQuerie = dfCovidClean2.join(\n",
    "    vaccinationsClean,$\"date\" === $\"dateVaccinated\"\n",
    "    && dfCovidClean2(\"Country/Region\") <=> vaccinationsClean(\"country\")\n",
    ").join(dfPopulation,\"country\")\n",
    "        .select($\"country\",\n",
    "                $\"date\",\n",
    "                $\"confirmed\",\n",
    "                $\"people_vaccinated\",\n",
    "                $\"Population\",\n",
    "                $\"confirmed\" / $\"Population\" as \"infection Per Population\",\n",
    "                $\"people_vaccinated\"/ $\"Population\" as \"vaccination Per Population\",\n",
    "                $\"people_vaccinated\" / $\"confirmed\" as \"infection-vaccination rate\")\n",
    "        .orderBy($\"date\".asc)\n",
    "        .na.fill(\"\")\n",
    "        .withColumn(\"infection-vaccination rate\", round($\"infection-vaccination rate\",8))\n",
    "        .withColumn(\"vaccination Per Population\", round($\"vaccination Per Population\",8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    megaQuerie.collect\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consulta utilizando los datos en .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*dfCovid.write\n",
    "    .partitionBy(\"countriesAndTerritories\",\"cases\")\n",
    "    .parquet(\"data_files/covid.parquet\")\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val parqDF = spark.read.parquet(\"data_files/covid.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "casos por km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val parquetCasesKM2 =\n",
    "parqDF.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "        .select($\"country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"country\")\n",
    "        .avg(\"infection Per Km\\u00b2\")\n",
    "        .orderBy(desc(\"avg(infection Per Km²)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    parquetCasesKM2\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    parquetCasesKM2\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val parquetCasesPopulation =\n",
    "parqDF.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "        .select($\"country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Population\",\n",
    "                $\"cases\" / $\"Population\" as \"infection Per Population\")\n",
    "        .groupBy(\"country\")\n",
    "        .avg(\"infection Per Population\")\n",
    "        .orderBy(desc(\"avg(infection Per Population)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    parquetCasesPopulation\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porcentaje diario de infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val parquetDailyCasesRate =\n",
    "parqDF.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "                .select($\"country\",\n",
    "                $\"dateRep\",\n",
    "                $\"day\",\n",
    "                $\"month\",\n",
    "                $\"cases\",\n",
    "                $\"Population\",\n",
    "                $\"cases\" / $\"Population\" as \"infection Per Population\")\n",
    "        .orderBy($\"dateRep\".asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    parquetDailyCasesRate\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de datos con plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x, y) = Seq(\n",
    "  \"Banana\" -> 10,\n",
    "  \"Apple\" -> 8,\n",
    "  \"Grapefruit\" -> 5\n",
    ").unzip\n",
    "\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## media de infecciones diarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val (x,y) = infAvgOrDf.collect.map(r=>(r(0).toString, r(1).toString.toDouble)).toList.unzip\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## media de infecciones por km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val (x,y) = meanInfectionsperKM2DF.collect.map(r=>(r(0).toString, r(1).toString.toFloat)).toList.unzip\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## media de infecciones por densidad de población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val (x,y) = infectionsPerPopulation.collect.map(r=>(r(0).toString, r(1).toString)).toList.unzip\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## porcentaje diario de infectados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x,y) = diaryInfectionsDF.filter($\"country\" === \"Spain\").collect.map(r=>(r(1).toString, r(6).toString)).toList.unzip\n",
    "Bar(x, y).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparacion entre paises de crecimiento de la enfermedad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val y = diaryInfectionsDF.filter($\"country\" === \"Spain\").select($\"infection Per Population\").\n",
    "    collect.map(r => r(0).toString.toDouble).toList\n",
    "\n",
    "val x = diaryInfectionsDF.filter($\"country\" === \"Spain\").select($\"to_date\").collect.toList.map(_.toString)\n",
    "\n",
    "val y1 = diaryInfectionsDF.filter($\"country\" === \"Italy\").select($\"infection Per Population\").\n",
    "    collect.map(r => r(0).toString.toDouble).toList\n",
    "val x1 = diaryInfectionsDF.filter($\"country\" === \"Italy\").select($\"to_date\").collect.toList.map(_.toString)\n",
    "\n",
    "val data = Seq(\n",
    "    Scatter(x,y).withName(\"Spain\"),\n",
    "    Scatter(x1,y1,mode = ScatterMode(ScatterMode.Lines),\n",
    "  line = Line(color = Color.StringColor(\"#7F7F7F\"))).withName(\"Italy\")\n",
    ").map(_.withFill(Fill.ToNextY).withStackgroup(\"A\"))\n",
    "\n",
    "plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## porcentaje de la población vacunada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crecimiento de la vacunacion con respecto a la población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val y = megaQuerie.filter($\"country\" === \"Chile\").select($\"vaccination Per Population\" * 10000000).\n",
    "    collect.map(r => r(0).toString.toDouble).toList\n",
    "\n",
    "val x = megaQuerie.filter($\"country\" === \"Chile\").select($\"date\").collect.toList.map(_.toString)\n",
    "\n",
    "val y1 = megaQuerie.filter($\"country\" === \"Chile\").select($\"people_vaccinated\").\n",
    "    collect.map(r => r(0).toString.toDouble).toList\n",
    "val x1 = megaQuerie.filter($\"country\" === \"Chile\").select($\"date\").collect.toList.map(_.toString)\n",
    "\n",
    "val data = Seq(\n",
    "    Scatter(x,y).withName(\"% population\"),\n",
    "    Scatter(x1,y1).withName(\"Vaccines administrated\")\n",
    ").map(_.withFill(Fill.ToNextY).withStackgroup(\"A\"))\n",
    "\n",
    "val myLayout =\n",
    "  Layout()\n",
    "    .withTitle(\"CHILE\")\n",
    "\n",
    "plot(data,myLayout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de eficiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para la querie de media de infecciones diarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x, y) = Seq(\n",
    "    \"RDD\" -> runWithOutput(infectionAvgRDD.collect),\n",
    "    \"DataSet\" -> runWithOutput(avgDS.collect),\n",
    "    \"DataFrame\" -> runWithOutput(infAvgOrDf.collect)\n",
    ").unzip\n",
    "\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para la querie de infecciones por km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x, y) = Seq(\n",
    "    \"Not Optimized RDD\" -> runWithOutput(notOptimizedRDD.collect),\n",
    "    \"RDD\" -> runWithOutput(meanInfectionsRDD.collect),\n",
    "    \"DataSet\" -> runWithOutput(meanInfectionsperKM2DS.collect),\n",
    "    \"DataFrame\" -> runWithOutput(meanInfectionsperKM2DF.collect),\n",
    "    \"DataFrame using Parquet\" -> runWithOutput(parquetCasesKM2.collect)\n",
    ").unzip\n",
    "\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para la querie de infecciones por número de habitantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x, y) = Seq(\n",
    "    \"DataSet\" -> runWithOutput(meanInfectionPerPopulationDS.collect),\n",
    "    \"DataFrame\" -> runWithOutput(infectionsPerPopulation.collect),\n",
    "    \"DataFrame vaccinations\" -> runWithOutput(megaQuerie.collect),\n",
    "    \"DataFrame using Parquet\" -> runWithOutput(parquetCasesPopulation.collect)\n",
    ").unzip\n",
    "\n",
    "Bar(x, y).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
