{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de Consultas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creamos la sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "\n",
    "import org.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
    "\n",
    "val spark: SparkSession = \n",
    "    NotebookSparkSession\n",
    "      .builder()\n",
    "      .appName(\"Queries Optimization\")\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.8.1`\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`ch.cern.sparkmeasure:spark-measure_2.12:0.17`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "import spark.sqlContext.implicits._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructType}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.types._, func._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset ha sido obtenido de:\n",
    "https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide\n",
    "\n",
    "En el se observan los casos diarios de Covid-19 por país hasta el 14-12-20\n",
    "\n",
    "En la segunda parte se utilizan los datos de las medidas aplicadas a cada país por fecha de inicio y fin:\n",
    "\n",
    "https://www.ecdc.europa.eu/en/publications-data/download-data-response-measures-covid-19\n",
    "\n",
    "La última consulta para calcular las infecciones por km2:\n",
    "\n",
    "https://www.kaggle.com/tanuprabhu/population-by-country-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo una clase para trabajar con infecciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Infection(day : Int, \n",
    "                     month : Int, \n",
    "                     year : Int, \n",
    "                     nCases: Int, \n",
    "                     nDeaths : Int, \n",
    "                     country : String,  \n",
    "                     continent : String) \n",
    "extends Serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y un método para medir tiempos de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run[A](code: => A): A = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    println(s\"Took ${System.currentTimeMillis() - start}\")\n",
    "    res\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runWithOutput[A](code: => A): Int = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    val out = System.currentTimeMillis() - start\n",
    "    println(s\"Took ${System.currentTimeMillis() - start}\")\n",
    "    out.toInt\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para utilizar showHTML() no he descubierto como importarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Credit to Aivean\n",
    "implicit class RichDF(val ds:DataFrame) {\n",
    "    def showHTML(limit:Int = 20, truncate: Int = 20) = {\n",
    "        import xml.Utility.escape\n",
    "        val data = ds.take(limit)\n",
    "        val header = ds.schema.fieldNames.toSeq        \n",
    "        val rows: Seq[Seq[String]] = data.map { row =>\n",
    "          row.toSeq.map { cell =>\n",
    "            val str = cell match {\n",
    "              case null => \"null\"\n",
    "              case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\")\n",
    "              case array: Array[_] => array.mkString(\"[\", \", \", \"]\")\n",
    "              case seq: Seq[_] => seq.mkString(\"[\", \", \", \"]\")\n",
    "              case _ => cell.toString\n",
    "            }\n",
    "            if (truncate > 0 && str.length > truncate) {\n",
    "              // do not show ellipses for strings shorter than 4 characters.\n",
    "              if (truncate < 4) str.substring(0, truncate)\n",
    "              else str.substring(0, truncate - 3) + \"...\"\n",
    "            } else {\n",
    "              str\n",
    "            }\n",
    "          }: Seq[String]\n",
    "        }\n",
    "publish.html(s\"\"\" <table>\n",
    "                <tr>\n",
    "                 ${header.map(h => s\"<th>${escape(h)}</th>\").mkString}\n",
    "                </tr>\n",
    "                ${rows.map { row =>\n",
    "                  s\"<tr>${row.map{c => s\"<td>${escape(c)}</td>\" }.mkString}</tr>\"\n",
    "                }.mkString}\n",
    "            </table>\n",
    "        \"\"\")        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcularemos la media usando solo Scala sin que sea distribuido\n",
    "(Work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def infectionsScala(lines : List[String]) : List[Infection] =\n",
    "    lines.map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Infection(day = arr(1).toInt,\n",
    "        month = arr(2).toInt,\n",
    "        year = arr(3).toInt,\n",
    "        nCases = arr(4).toInt,\n",
    "        nDeaths = arr(5).toInt,\n",
    "        country = arr(6),\n",
    "        continent = arr(10))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val content= scala.io.Source.fromFile(\"data.csv\").getLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// infectionsScala(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*  def infectionArrayGrowthAverage(infections : List[Infection]) : Map[String, Int]= {\n",
    "\n",
    "    val countriesAndCases : Map[String, List[(String, Int)]] = \n",
    "      infections.map(x => (x.country,x.nCases))\n",
    "      .groupBy(_._1)\n",
    "      \n",
    "    countriesAndCases.mapValues(x => (x.sum / x.size))\n",
    "  }\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* val bufferedSource = scala.io.Source.fromFile(\"data.csv\")\n",
    "  for (line <- bufferedSource.getLines) {\n",
    "    val cols = line.split(\",\").map(_.trim)\n",
    "\n",
    "      // do whatever you want with the columns here\n",
    "      \n",
    "    println(s\"${cols(0)}|${cols(1)}|${cols(2)}|${cols(3)}|${cols(4)}|${cols(5)}|${cols(6)}\")\n",
    "  }\n",
    "  bufferedSource.close\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empiezo trabajando con RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionData = spark.sparkContext.textFile(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una funcion para trabajar con un RDD de infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infections(lines : RDD[String]) : RDD[Infection] =\n",
    "    lines.map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Infection(\n",
    "        day = arr(1).toInt,\n",
    "        month = arr(2).toInt,\n",
    "        year = arr(3).toInt,\n",
    "        nCases = arr(4).toInt,\n",
    "        nDeaths = arr(5).toInt,\n",
    "        country = arr(6),\n",
    "        continent = arr(10)\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo la media de infecciones diarias por país trabajando con pair RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def infectionGrowthAverage(infections : RDD[Infection]) : RDD[(String, Int)]= {\n",
    "\n",
    "    val countriesAndCases : RDD[(String, Iterable[Int])] = \n",
    "      infections.map(x => (x.country,x.nCases))\n",
    "      .groupByKey()\n",
    "      \n",
    "    countriesAndCases.mapValues(x => (x.sum / x.size)).sortBy(_._2)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestro el resultado y el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionRDD = infections(infectionData)\n",
    "val infectionAvgRDD = infectionGrowthAverage(infectionRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val timeRDD = spark.time(infectionAvgRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(infectionAvgRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hago los mismos calculos con un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierto el RDD obtenido previamente en un DataFrame para inferir la clase infección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDF = spark.createDataFrame(infectionRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizo los métodos de la clase DF que incluye uno optimizado para calcular la media.\n",
    "\n",
    "Ejecuto y comprabamos como el tiempo de ejecución es significativamente menor que en RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infAvgOrDf = infectionDF.\n",
    "    groupBy(\"country\")\n",
    "    .avg(\"nCases\")\n",
    "    .orderBy(desc(\"avg(nCases)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "infAvgOrDf.showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time(infAvgOrDf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val timeDF = spark.time(infAvgOrDf.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(infAvgOrDf.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es crear el DataFrame directamente importando los datos pero deja de ser un DF de infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovid = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"covidworldwide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dfCovid.toDF.groupBy(\"countriesAndTerritories\")\n",
    "    .agg(mean(\"cases\")).orderBy(\"avg(cases)\")).show(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "puedo definir el esquema manualmente para crear el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Defino el esquema manualmente pero podría verlo importando el csv y viendo como lo hace de base spark\n",
    "\n",
    "val schema = new StructType()\n",
    "    .add(\"dateRep\",StringType,true)\n",
    "    .add(\"day\",IntegerType,true)\n",
    "    .add(\"month\",IntegerType,true)\n",
    "    .add(\"year\",IntegerType,true)\n",
    "    .add(\"cases\",IntegerType,true)\n",
    "    .add(\"deaths\",IntegerType,true)\n",
    "    .add(\"countriesAndTerritories\",StringType,true)\n",
    "    .add(\"geoId\",StringType,true)\n",
    "    .add(\"countryterritoryCode\",StringType,true)\n",
    "    .add(\"popData2018\",IntegerType,true)\n",
    "    .add(\"continentExp\",StringType,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read\n",
    ".format(\"csv\")\n",
    ".option(\"header\",\"true\")\n",
    ".schema(schema)\n",
    ".load(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y con un DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".csv(\"covidworldwide.csv\")\n",
    ".as[(String,String,String,String,String,String,String,String,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val avgDS = \n",
    "    infectionDS.groupBy($\"countriesAndTerritories\")\n",
    "    .agg(avg($\"cases\").as[Double])\n",
    "    .orderBy(\"avg(cases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time(avgDS.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val timeDS = spark.time(avgDS.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val timeDataSet = runWithOutput(avgDS.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras opciones menos eficientes\n",
    "(Work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* infectionDS.groupByKey(p => p._7) //hace shuffling de los datos\n",
    "        .mapValues(p => p._5.toDouble)\n",
    "        .mapGroups((k,vs) => (???))\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* infectionDS.groupByKey(p => p._7) //mas eficiente\n",
    "        .mapValues(p => p._5.toDouble)\n",
    "        .reduceGroups((acc,str)=> ???)\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* val infGrowAvg = new Aggregator[]{ //utilizando un Aggregator\n",
    "    \n",
    "}.toColumn \n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento trabajar con DataSet de infecciones Dataset[Infection] pero falla \n",
    "(Work in progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos dará error pues no se pueden crear datasets sin tipo de datos y no entiende las infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.createDataset(infectionRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.createDataset(infectionRDD).as[Infection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds : Dataset[Infection] = spark.createDataset(infectionRDD).as[Infection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesito importar los Encoders y explicitar el tipo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Encoders\n",
    "Encoders.product[Infection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{Encoder, Encoders}\n",
    "import org.apache.spark.sql.Encoders\n",
    "\n",
    "val dataset = spark.createDataset(infectionRDD)(Encoders.product[Infection])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A partir de aquí intento crear un DataSet de infecciones pero siempre obtengo el mismo error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.createDataset(infectionRDD).as[Infection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def infections(lines : Dataset[(String,String,String,String,String,String,String,String,String,String,String,String)]) \n",
    "                       : Dataset[Infection] = \n",
    "    lines.map(line => {\n",
    "      Infection(\n",
    "        day = line._1.toInt,\n",
    "        month = line._2.toInt,\n",
    "        year = line._3.toInt,\n",
    "        nCases = line._4.toInt,\n",
    "        nDeaths = line._5.toInt,\n",
    "        country = line._6,\n",
    "        continent = line._10\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infections(infectionDS).as[Infection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizo una segunda tabla y cruzo datos con RDD, DS y DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo una consulta para calcular la media de infecciones por Km2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationData = spark.sparkContext.textFile(\"population_by_country_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Population(\n",
    "    country : String, \n",
    "    population : Int, \n",
    "    density : Int, \n",
    "    land_area: Int, \n",
    "    ) \n",
    "extends Serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpio la primera linea del CSV y creo un RDD de población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val header = populationData.first() \n",
    "\n",
    "def population(lines : RDD[String]) : RDD[Population] =\n",
    "    lines.filter(x => x != header)\n",
    "    .map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Population(\n",
    "        country = arr(0),\n",
    "        population = arr(1).toInt,\n",
    "        density = arr(4).toInt,\n",
    "        land_area = arr(5).toInt,\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compruebo que se visualizan correctamente los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationRDD = population(populationData)\n",
    "populationRDD.toDF.showHTML()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un join computacionalmente pesado desde el principio ya que cruza todos los datos sin quedarnos con los que nos interesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark no me deja hacer un Join de RDD que no sean pair RDD así que tenemos que construirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populationRDD.join(infectionRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyo Pair RDDs conservando todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val populationByCountry = populationRDD.map(\n",
    "    x => (x.country,x))\n",
    "\n",
    "val infectionByCountry = \n",
    "      infectionRDD.map(x => (x.country,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago el Join y agrupo por paises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val megaRDD = infectionByCountry.join(populationByCountry).groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente calculo la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megaRDD.mapValues(\n",
    "    x => x.map( \n",
    "        line => line._1.nCases.toFloat / line._2.land_area.toFloat\n",
    "    )).mapValues(\n",
    "    x => x.sum / x.size\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo hago todo en una única operación para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    infectionByCountry.join(populationByCountry)\n",
    "    .groupByKey()\n",
    "    .mapValues(\n",
    "    x => x.map( \n",
    "        line => line._1.nCases.toFloat / line._2.land_area.toFloat)\n",
    "    ).mapValues(\n",
    "        x => x.sum / x.size\n",
    "    ).collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Hay alguna diferencia cruzando los datos en orden inverso? Parece que no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "    populationByCountry.join(infectionByCountry)\n",
    "    .groupByKey()\n",
    "    .mapValues(\n",
    "    x => x.map( \n",
    "        line => line._1.land_area.toFloat / line._2.nCases.toFloat)\n",
    "    ).mapValues(\n",
    "        x => x.sum / x.size\n",
    "    ).collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para optimizar un poco esta consulta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despejo solo los datos que me interesan para trabajar con Pair RDDs y optimizar la consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val countriesAndLandArea = populationRDD.map(\n",
    "    x => (x.country,x.land_area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val countriesAndCases = \n",
    "      infectionRDD.map(x => (x.country,x.nCases))\n",
    "      .groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuto un join y trabajo para calcular primero la media de infecciones por Km2 diaria, \n",
    "para luego calcular la media total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val average = countriesAndCases.join(countriesAndLandArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average.mapValues(\n",
    "    x => x._1.map(\n",
    "        y => (y.toFloat / x._2.toFloat)\n",
    "    )).mapValues(\n",
    "    x => x.sum/x.size\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo hago todo en una única operación para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val querie1 =\n",
    "countriesAndCases.join(countriesAndLandArea)   \n",
    ".mapValues(\n",
    "    x => x._1.map(\n",
    "        y => (y.toFloat / x._2.toFloat)\n",
    "    )).mapValues(\n",
    "    x => x.sum / x.size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "countriesAndCases.join(countriesAndLandArea)   \n",
    ".mapValues(\n",
    "    x => x._1.map(\n",
    "        y => (y.toFloat / x._2.toFloat)\n",
    "    )).mapValues(\n",
    "    x => x.sum / x.size\n",
    ").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas : producto cartesiano y demás (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populationRDD.cartesian(infectionRDD).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val joined = infectionAvgRDD.join(countriesAndLandArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infectionAvgRDD.collect()\n",
    "countriesAndLandArea.collect()\n",
    "joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.map( x => (x._1,(x._2._1 / x._2._2) : Float)).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta con DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"covidworldwide.csv\")\n",
    ".as[(String,String,String,String,Double,Double,String,String,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dsPopulation = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"population_by_country_2020.csv\")\n",
    ".as[(String,Float,String,Float,Float,Float,Double,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val querie2 = \n",
    "infectionDS.join(dsPopulation, $\"Country (or dependency)\" === $\"countriesAndTerritories\")\n",
    "        .select($\"Country (or dependency)\" as \"Country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"Country\")\n",
    "        .avg(\"infection Per Km\\u00b2\")\n",
    "        .orderBy(asc(\"avg(infection Per Km²)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "infectionDS.join(dsPopulation, $\"Country (or dependency)\" === $\"countriesAndTerritories\")\n",
    "        .select($\"Country (or dependency)\" as \"Country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"Country\")\n",
    "        .avg(\"infection Per Km\\u00b2\")\n",
    "        .orderBy(asc(\"avg(infection Per Km²)\"))\n",
    "        .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intentando dataset mejor tipado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class InfectionRow(\n",
    "        date : String,\n",
    "        day : String,\n",
    "        month : String,\n",
    "        year : String,\n",
    "        nCases: Double,\n",
    "        nDeaths : Double,\n",
    "        country : String,\n",
    "        geoID :String,\n",
    "        territoryCode : String,\n",
    "        population :Int,\n",
    "        continent : String,\n",
    "        cumulative_14_days : Float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infectionDSTyped(infectionDF: DataFrame): Dataset[InfectionRow] =\n",
    "    infectionDF.map(r => InfectionRow(\n",
    "      r.getAs[String](\"dateRep\"),\n",
    "      r.getAs[String](\"day\"),\n",
    "      r.getAs[String](\"month\"),\n",
    "      r.getAs[String](\"year\"),\n",
    "      r.getAs[Double](\"nCases\"),\n",
    "      r.getAs[Double](\"nDeaths\"),\n",
    "      r.getAs[String](\"Country\"),\n",
    "      r.getAs[String](\"geoID\"),\n",
    "      r.getAs[String](\"territoryCode\"),\n",
    "      r.getAs[Int](\"population\"),\n",
    "      r.getAs[String](\"continent\"),\n",
    "      r.getAs[Float](\"cumulative_14_days\")\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infectionDSTyped(infectionDS).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infectionDS.join(dsPopulation, \"Country\" ).showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val querie2 = \n",
    "infectionDS.join(dsPopulation, \"Country\")\n",
    "        .map(r => \n",
    "             (r.getString(0),r.getTimestamp(1),r.getInt(5),r.getInt(16),r.getInt(16)/r.getInt(16)))\n",
    "        .groupByKey()\n",
    "        .agg(round(avg(_._5),10).as[Float])\n",
    "        .orderBy(desc(_._5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infectionDS.join(dsPopulation, \"Country\")\n",
    "        .map(r => \n",
    "             (r.getString(0),r.getString(1),r.getInt(5),r.getInt(16),r.getInt(16)/r.getInt(16)))\n",
    "        .groupByKey(_._1)\n",
    "        .mapValues(mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta con DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovid = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"covidworldwide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfMeasures = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"response_graphs_data_2021-04-15.csv\")\n",
    "dfMeasures.show\n",
    "dfMeasures.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfPopulation = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"population_by_country_2020.csv\")\n",
    ".withColumnRenamed(\"Country (or dependency)\",\"Country\")\n",
    ".withColumnRenamed(\"Population (2020)\",\"Population\")\n",
    "dfPopulation.showHTML()\n",
    "dfPopulation.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifico los datos de entrada para que el formato fecha se adecue al TimeStamp de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovidClean = dfCovid.select($\"*\",$\"dateRep\",translate($\"dateRep\",\"/\",\"-\").as(\"new-date\")).drop(\"dateRep\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago una consulta de prueba para obtener la media solo de los casos en España"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spainCovid = dfCovid.select(\"dateRep\",\"cases\").where(\"countriesAndTerritories == 'Spain'\").toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(spainCovid.agg(avg(\"cases\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cruzo los datos con un Join y hago algunas consultas sencillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val megaDF = dfCovid.join(dfMeasures, $\"Country\" === $\"countriesAndTerritories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megaDF.select(\"cases\",\"deaths\",\"dateRep\",\"Response_measure\")\n",
    "    .where(\"countriesAndTerritories == 'Spain'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dfCovid.join(dfMeasures, $\"Country\" === $\"countriesAndTerritories\")\n",
    "        .select(\"cases\",\"deaths\",\"dateRep\",\"Response_measure\")\n",
    "        .where(\"countriesAndTerritories == 'Spain'\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente ejecuto la consulta de nuestro caso de uso, infecciones por Km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val querie3 = \n",
    "dfCovid.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "        .select($\"country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"country\")\n",
    "        .avg(\"infection Per Km\\u00b2\")\n",
    "        .orderBy(desc(\"avg(infection Per Km²)\"))\n",
    "        .collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(\n",
    "dfCovid.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "        .select($\"country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"country\")\n",
    "        .avg(\"infection Per Km\\u00b2\")\n",
    "        .orderBy(desc(\"avg(infection Per Km²)\"))\n",
    "        .collect()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casos por número de habitante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionsPerPopulation = dfCovid.join(dfPopulation, $\"country\" === $\"countriesAndTerritories\")\n",
    "        .select($\"country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Population\",\n",
    "                $\"cases\" / $\"Population\" as \"infection Per Population\")\n",
    "        .groupBy(\"country\")\n",
    "        .avg(\"infection Per Population\")\n",
    "        .orderBy(desc(\"avg(infection Per Population)\"))\n",
    "        .collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas y observaciones personales interesantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.select(\"dateRep\",\"countriesAndTerritories\",\"cases\").show //aplico los métodos de DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.createOrReplaceTempView(\"covid\") //se pueden aplicar consultas SQL sobre DF\n",
    "\n",
    "spark.sql(\"SELECT * FROM covid WHERE countriesAndTerritories == 'Spain'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//filtrados sobre dataframes\n",
    "dfCovid.filter($\"continentExp\" === \"Asia\" || $\"continentExp\" === \"Europe\").sort($\"continentExp\".asc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de datos con plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x, y) = Seq(\n",
    "  \"Banana\" -> 10,\n",
    "  \"Apple\" -> 8,\n",
    "  \"Grapefruit\" -> 5\n",
    ").unzip\n",
    "\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val (x,y) = infAvgOrDf.collect.map(r=>(r(0).toString, r(1).toString.toDouble)).toList.unzip\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val (x,y) = querie3.map(r=>(r(0).toString, r(1).toString.toFloat)).toList.unzip\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val (x,y) = infectionsPerPopulation.map(r=>(r(0).toString, r(1).toString)).toList.unzip\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de eficiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para la querie de media de infecciones diarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x, y) = Seq(\n",
    "  \"RDD\" -> runWithOutput(infectionAvgRDD.collect),\n",
    "  \"DataSet\" -> runWithOutput(infAvgOrDf.collect),\n",
    "  \"DataFrame\" -> runWithOutput(avgDS.collect)\n",
    ").unzip\n",
    "\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para la querie de infecciones por km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x, y) = Seq(\n",
    "  \"RDD\" -> runWithOutput(querie1.collect),\n",
    "  \"DataSet\" -> runWithOutput(querie2.collect),\n",
    "  \"DataFrame\" -> runWithOutput(querie3.collect)\n",
    ").unzip\n",
    "\n",
    "Bar(x, y).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
