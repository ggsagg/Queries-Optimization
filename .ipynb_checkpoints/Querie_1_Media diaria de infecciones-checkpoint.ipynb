{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/06/18 07:26:38 WARN Utils: Your hostname, MP-5CG0326Q5C resolves to a loopback address: 127.0.1.1; using 172.24.62.209 instead (on interface eth0)\n",
      "21/06/18 07:26:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/06/18 07:26:39 INFO SparkContext: Running Spark version 2.4.5\n",
      "21/06/18 07:26:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/06/18 07:26:40 INFO SparkContext: Submitted application: Queries Optimization\n",
      "21/06/18 07:26:40 INFO SecurityManager: Changing view acls to: jserrano\n",
      "21/06/18 07:26:40 INFO SecurityManager: Changing modify acls to: jserrano\n",
      "21/06/18 07:26:40 INFO SecurityManager: Changing view acls groups to: \n",
      "21/06/18 07:26:40 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/06/18 07:26:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jserrano); groups with view permissions: Set(); users  with modify permissions: Set(jserrano); groups with modify permissions: Set()\n",
      "21/06/18 07:26:40 INFO Utils: Successfully started service 'sparkDriver' on port 41625.\n",
      "21/06/18 07:26:40 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/06/18 07:26:40 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/06/18 07:26:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/06/18 07:26:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/06/18 07:26:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-42a09e40-6be3-47fb-84d0-96bdbaa5dbb3\n",
      "21/06/18 07:26:40 INFO MemoryStore: MemoryStore started with capacity 1711.2 MB\n",
      "21/06/18 07:26:40 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/06/18 07:26:40 INFO Utils: Successfully started service 'SparkUI' on port 4043.\n",
      "21/06/18 07:26:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.24.62.209:4043\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0-sources.jar at spark://172.24.62.209:41625/jars/jvm-repr-0.4.0-sources.jar with timestamp 1623994001086\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0.jar at spark://172.24.62.209:41625/jars/jvm-repr-0.4.0.jar with timestamp 1623994001087\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1-sources.jar at spark://172.24.62.209:41625/jars/scopt_2.12-3.7.1-sources.jar with timestamp 1623994001088\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1.jar at spark://172.24.62.209:41625/jars/scopt_2.12-3.7.1.jar with timestamp 1623994001088\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.11/2.2.0-4-4bd225e/ammonite-interp-api_2.12.11-2.2.0-4-4bd225e-sources.jar at spark://172.24.62.209:41625/jars/ammonite-interp-api_2.12.11-2.2.0-4-4bd225e-sources.jar with timestamp 1623994001089\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.11/2.2.0-4-4bd225e/ammonite-interp-api_2.12.11-2.2.0-4-4bd225e.jar at spark://172.24.62.209:41625/jars/ammonite-interp-api_2.12.11-2.2.0-4-4bd225e.jar with timestamp 1623994001090\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/2.2.0-4-4bd225e/ammonite-ops_2.12-2.2.0-4-4bd225e-sources.jar at spark://172.24.62.209:41625/jars/ammonite-ops_2.12-2.2.0-4-4bd225e-sources.jar with timestamp 1623994001092\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/2.2.0-4-4bd225e/ammonite-ops_2.12-2.2.0-4-4bd225e.jar at spark://172.24.62.209:41625/jars/ammonite-ops_2.12-2.2.0-4-4bd225e.jar with timestamp 1623994001094\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.11/2.2.0-4-4bd225e/ammonite-repl-api_2.12.11-2.2.0-4-4bd225e-sources.jar at spark://172.24.62.209:41625/jars/ammonite-repl-api_2.12.11-2.2.0-4-4bd225e-sources.jar with timestamp 1623994001095\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.11/2.2.0-4-4bd225e/ammonite-repl-api_2.12.11-2.2.0-4-4bd225e.jar at spark://172.24.62.209:41625/jars/ammonite-repl-api_2.12.11-2.2.0-4-4bd225e.jar with timestamp 1623994001097\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/2.2.0-4-4bd225e/ammonite-util_2.12-2.2.0-4-4bd225e-sources.jar at spark://172.24.62.209:41625/jars/ammonite-util_2.12-2.2.0-4-4bd225e-sources.jar with timestamp 1623994001098\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/2.2.0-4-4bd225e/ammonite-util_2.12-2.2.0-4-4bd225e.jar at spark://172.24.62.209:41625/jars/ammonite-util_2.12-2.2.0-4-4bd225e.jar with timestamp 1623994001098\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.9/fansi_2.12-0.2.9-sources.jar at spark://172.24.62.209:41625/jars/fansi_2.12-0.2.9-sources.jar with timestamp 1623994001100\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.9/fansi_2.12-0.2.9.jar at spark://172.24.62.209:41625/jars/fansi_2.12-0.2.9.jar with timestamp 1623994001101\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.6.2/geny_2.12-0.6.2-sources.jar at spark://172.24.62.209:41625/jars/geny_2.12-0.6.2-sources.jar with timestamp 1623994001102\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.6.2/geny_2.12-0.6.2.jar at spark://172.24.62.209:41625/jars/geny_2.12-0.6.2.jar with timestamp 1623994001103\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.7.1/os-lib_2.12-0.7.1-sources.jar at spark://172.24.62.209:41625/jars/os-lib_2.12-0.7.1-sources.jar with timestamp 1623994001104\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.7.1/os-lib_2.12-0.7.1.jar at spark://172.24.62.209:41625/jars/os-lib_2.12-0.7.1.jar with timestamp 1623994001108\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.9/pprint_2.12-0.5.9-sources.jar at spark://172.24.62.209:41625/jars/pprint_2.12-0.5.9-sources.jar with timestamp 1623994001111\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.9/pprint_2.12-0.5.9.jar at spark://172.24.62.209:41625/jars/pprint_2.12-0.5.9.jar with timestamp 1623994001113\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.2.1/sourcecode_2.12-0.2.1-sources.jar at spark://172.24.62.209:41625/jars/sourcecode_2.12-0.2.1-sources.jar with timestamp 1623994001114\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.2.1/sourcecode_2.12-0.2.1.jar at spark://172.24.62.209:41625/jars/sourcecode_2.12-0.2.1.jar with timestamp 1623994001116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/0.0.25/interface-0.0.25-sources.jar at spark://172.24.62.209:41625/jars/interface-0.0.25-sources.jar with timestamp 1623994001118\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/0.0.25/interface-0.0.25.jar at spark://172.24.62.209:41625/jars/interface-0.0.25.jar with timestamp 1623994001119\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.2.0/scala-collection-compat_2.12-2.2.0-sources.jar at spark://172.24.62.209:41625/jars/scala-collection-compat_2.12-2.2.0-sources.jar with timestamp 1623994001119\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.2.0/scala-collection-compat_2.12-2.2.0.jar at spark://172.24.62.209:41625/jars/scala-collection-compat_2.12-2.2.0.jar with timestamp 1623994001119\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.3.0/scala-xml_2.12-1.3.0-sources.jar at spark://172.24.62.209:41625/jars/scala-xml_2.12-1.3.0-sources.jar with timestamp 1623994001120\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.3.0/scala-xml_2.12-1.3.0.jar at spark://172.24.62.209:41625/jars/scala-xml_2.12-1.3.0.jar with timestamp 1623994001120\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.11/scala-compiler-2.12.11-sources.jar at spark://172.24.62.209:41625/jars/scala-compiler-2.12.11-sources.jar with timestamp 1623994001121\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.11/scala-library-2.12.11-sources.jar at spark://172.24.62.209:41625/jars/scala-library-2.12.11-sources.jar with timestamp 1623994001122\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11-sources.jar at spark://172.24.62.209:41625/jars/scala-reflect-2.12.11-sources.jar with timestamp 1623994001122\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.10.9/interpreter-api_2.12-0.10.9-sources.jar at spark://172.24.62.209:41625/jars/interpreter-api_2.12-0.10.9-sources.jar with timestamp 1623994001123\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.10.9/interpreter-api_2.12-0.10.9.jar at spark://172.24.62.209:41625/jars/interpreter-api_2.12-0.10.9.jar with timestamp 1623994001123\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.10.9/jupyter-api_2.12-0.10.9-sources.jar at spark://172.24.62.209:41625/jars/jupyter-api_2.12-0.10.9-sources.jar with timestamp 1623994001123\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.10.9/jupyter-api_2.12-0.10.9.jar at spark://172.24.62.209:41625/jars/jupyter-api_2.12-0.10.9.jar with timestamp 1623994001124\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.11/0.10.9/scala-kernel-api_2.12.11-0.10.9-sources.jar at spark://172.24.62.209:41625/jars/scala-kernel-api_2.12.11-0.10.9-sources.jar with timestamp 1623994001124\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.11/0.10.9/scala-kernel-api_2.12.11-0.10.9.jar at spark://172.24.62.209:41625/jars/scala-kernel-api_2.12.11-0.10.9.jar with timestamp 1623994001125\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.12/scala-library-2.12.12.jar at spark://172.24.62.209:41625/jars/scala-library-2.12.12.jar with timestamp 1623994001126\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/plotly-scala/plotly-almond_2.12/0.8.1/plotly-almond_2.12-0.8.1.jar at spark://172.24.62.209:41625/jars/plotly-almond_2.12-0.8.1.jar with timestamp 1623994001126\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/plotly-scala/plotly-core_2.12/0.8.1/plotly-core_2.12-0.8.1.jar at spark://172.24.62.209:41625/jars/plotly-core_2.12-0.8.1.jar with timestamp 1623994001127\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/plotly-scala/plotly-render_2.12/0.8.1/plotly-render_2.12-0.8.1.jar at spark://172.24.62.209:41625/jars/plotly-render_2.12-0.8.1.jar with timestamp 1623994001128\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/webjars/bower/plotly.js/1.52.2/plotly.js-1.52.2.jar at spark://172.24.62.209:41625/jars/plotly.js-1.52.2.jar with timestamp 1623994001131\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/ch/cern/sparkmeasure/spark-measure_2.12/0.17/spark-measure_2.12-0.17.jar at spark://172.24.62.209:41625/jars/spark-measure_2.12-0.17.jar with timestamp 1623994001132\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.9.9/jackson-module-scala_2.12-2.9.9.jar at spark://172.24.62.209:41625/jars/jackson-module-scala_2.12-2.9.9.jar with timestamp 1623994001133\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar at spark://172.24.62.209:41625/jars/slf4j-api-1.7.26.jar with timestamp 1623994001133\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/influxdb/influxdb-java/2.14/influxdb-java-2.14.jar at spark://172.24.62.209:41625/jars/influxdb-java-2.14.jar with timestamp 1623994001133\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.9.9/jackson-core-2.9.9.jar at spark://172.24.62.209:41625/jars/jackson-core-2.9.9.jar with timestamp 1623994001133\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.9.9/jackson-annotations-2.9.9.jar at spark://172.24.62.209:41625/jars/jackson-annotations-2.9.9.jar with timestamp 1623994001134\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.9.9/jackson-databind-2.9.9.jar at spark://172.24.62.209:41625/jars/jackson-databind-2.9.9.jar with timestamp 1623994001141\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-paranamer/2.9.9/jackson-module-paranamer-2.9.9.jar at spark://172.24.62.209:41625/jars/jackson-module-paranamer-2.9.9.jar with timestamp 1623994001142\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/retrofit2/retrofit/2.4.0/retrofit-2.4.0.jar at spark://172.24.62.209:41625/jars/retrofit-2.4.0.jar with timestamp 1623994001142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/retrofit2/converter-moshi/2.4.0/converter-moshi-2.4.0.jar at spark://172.24.62.209:41625/jars/converter-moshi-2.4.0.jar with timestamp 1623994001145\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/org/msgpack/msgpack-core/0.8.16/msgpack-core-0.8.16.jar at spark://172.24.62.209:41625/jars/msgpack-core-0.8.16.jar with timestamp 1623994001145\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/okhttp3/okhttp/3.11.0/okhttp-3.11.0.jar at spark://172.24.62.209:41625/jars/okhttp-3.11.0.jar with timestamp 1623994001146\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/okhttp3/logging-interceptor/3.11.0/logging-interceptor-3.11.0.jar at spark://172.24.62.209:41625/jars/logging-interceptor-3.11.0.jar with timestamp 1623994001146\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/moshi/moshi/1.5.0/moshi-1.5.0.jar at spark://172.24.62.209:41625/jars/moshi-1.5.0.jar with timestamp 1623994001146\n",
      "21/06/18 07:26:41 INFO SparkContext: Added JAR file:/home/jserrano/.cache/coursier/v1/https/repo1.maven.org/maven2/com/squareup/okio/okio/1.14.0/okio-1.14.0.jar at spark://172.24.62.209:41625/jars/okio-1.14.0.jar with timestamp 1623994001147\n",
      "21/06/18 07:26:41 INFO Executor: Starting executor ID driver on host localhost\n",
      "21/06/18 07:26:41 INFO Executor: Using REPL class URI: http://127.0.1.1:38907\n",
      "21/06/18 07:26:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39979.\n",
      "21/06/18 07:26:41 INFO NettyBlockTransferService: Server created on 172.24.62.209:39979\n",
      "21/06/18 07:26:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/06/18 07:26:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.24.62.209, 39979, None)\n",
      "21/06/18 07:26:41 INFO BlockManagerMasterEndpoint: Registering block manager 172.24.62.209:39979 with 1711.2 MB RAM, BlockManagerId(driver, 172.24.62.209, 39979, None)\n",
      "21/06/18 07:26:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.24.62.209, 39979, None)\n",
      "21/06/18 07:26:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.24.62.209, 39979, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://172.24.62.209:4043\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcommon._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{IntegerType, StringType, StructType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._, types._, functions._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.common\n",
    "import spark._\n",
    "import common._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructType}\n",
    "import org.apache.spark.sql._, types._, functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> esto lo puedes añadir a common.sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.Almond._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.8.1`\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD para la consulta de infecciones diarias por país"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minfectionData\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infectionData = spark.sparkContext.textFile(\"../datasets/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una funcion para trabajar con un RDD de infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minfections\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infections(lines : RDD[String]) : RDD[Infection] =\n",
    "    lines.map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Infection(\n",
    "        day = arr(1).toInt,\n",
    "        month = arr(2).toInt,\n",
    "        year = arr(3).toInt,\n",
    "        nCases = arr(4).toInt,\n",
    "        nDeaths = arr(5).toInt,\n",
    "        country = arr(6),\n",
    "        continent = arr(10)\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo la media de infecciones diarias por país trabajando con pair RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minfectionGrowthAverage\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  def infectionGrowthAverage(infections : RDD[Infection]) : RDD[(String, Int)]= {\n",
    "\n",
    "    val countriesAndCases : RDD[(String, Iterable[Int])] = \n",
    "      infections.map(x => (x.country,x.nCases))\n",
    "      .groupByKey()\n",
    "      \n",
    "    countriesAndCases.mapValues(x => (x.sum / x.size)).sortBy(_._2)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestro el resultado y el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minfectionRDD\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36minfectionAvgRDD\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infectionRDD = infections(infectionData)\n",
    "def infectionAvgRDD = infectionGrowthAverage(infectionRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> JM. Si te fijas en la SPark UI, se está lanzando ya un job debido al sortBy. El trabajo que resta por hacer es apenas nada. Por eso el tiempo que sale para el RDD es tan bajo. Cambia los `val infection*` por `def infection*`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la API de spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> JM. Ejecuta las queries solamente al final, cuando hagas la comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val timeRDD = spark.time(infectionAvgRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o bien el framework del cern que nos da más información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(infectionAvgRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hago los mismos calculos con un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> JM. No utilices esta opción de convertir el RDD en un DF. Utiliza una cualquiera de las otras que comentas más adelante (pero solo una)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierto el RDD obtenido previamente en un DataFrame para inferir la clase infección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minfectionDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [day: int, month: int ... 5 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val infectionDF = spark.createDataFrame(infectionRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizo los métodos de la API DF que incluye uno optimizado para calcular la media.\n",
    "\n",
    "Ejecuto y comprabamos el tiempo de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minfAvgOrDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [country: string, avg(nCases): double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val infAvgOrDf = infectionDF.\n",
    "    groupBy(\"country\")\n",
    "    .avg(\"nCases\")\n",
    "    .orderBy(desc(\"avg(nCases)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val timeDF = spark.time(infAvgOrDf.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(infAvgOrDf.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otra opción es crear el DataFrame directamente importando los datos pero deja de ser un DF de infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovid = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"../datasets/covidworldwide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.schema\n",
    "dfCovid.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovidWithSchema = dfCovid.toDF\n",
    "    .groupBy(\"countriesAndTerritories\")\n",
    "    .agg(mean(\"cases\"))\n",
    "    .orderBy(\"avg(cases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(dfCovidWithSchema.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O puedo definir el esquema manualmente para crear el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = new StructType()\n",
    "    .add(\"dateRep\",StringType,true)\n",
    "    .add(\"day\",IntegerType,true)\n",
    "    .add(\"month\",IntegerType,true)\n",
    "    .add(\"year\",IntegerType,true)\n",
    "    .add(\"cases\",IntegerType,true)\n",
    "    .add(\"deaths\",IntegerType,true)\n",
    "    .add(\"countriesAndTerritories\",StringType,true)\n",
    "    .add(\"geoId\",StringType,true)\n",
    "    .add(\"countryterritoryCode\",StringType,true)\n",
    "    .add(\"popData2018\",IntegerType,true)\n",
    "    .add(\"continentExp\",StringType,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read\n",
    ".format(\"csv\")\n",
    ".option(\"header\",\"true\")\n",
    ".schema(schema)\n",
    ".load(\"../datasets/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y con un DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minfectionDS\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infectionDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".csv(\"../datasets/covidworldwide.csv\")\n",
    ".as[(String,String,String,String,String,String,String,String,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mavgDS\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avgDS = \n",
    "    infectionDS.groupBy($\"countriesAndTerritories\")\n",
    "    .agg(avg($\"cases\"))\n",
    "    .orderBy(\"avg(cases)\")\n",
    "    .as[(String,Double)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(avgDS.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajamos con Dataset[Infection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDataset = spark.createDataset(infectionRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val avgInfectionDS = infectionDataset\n",
    "    .groupBy($\"country\")\n",
    "    .agg(avg($\"nCases\").as[Double])\n",
    "    .orderBy(\"avg(nCases)\")\n",
    "    .as[(String,Double)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch.cern.sparkmeasure.StageMetrics(spark).runAndMeasure(avgInfectionDS.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (x,y) = infAvgOrDf.collect.map(r=>(r(0).toString, r(1).toString.toDouble)).toList.unzip\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización de eficiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">map at cmd4.sc:4</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    2 / 2\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">sortBy at cmd4.sc:7</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    2 / 2\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">sortBy at cmd4.sc:7</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    2 / 2\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd13.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    2 / 2\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">csv at cmd9.sc:5</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd13.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    2 / 2\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd13.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    200 / 200\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd13.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    200 / 200\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd13.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    200 / 200\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2202\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd13.sc:4</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    200 / 200\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 652\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {},
       "data": [
        {
         "type": "bar",
         "x": [
          "RDD",
          "DataSet",
          "DataFrame"
         ],
         "y": [
          903,
          2202,
          652
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div class=\"chart\" id=\"plot-9aceb670-0d57-4e83-83c8-ba9962712f67\"></div>\n",
       "<script>require(['plotly'], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"x\":[\"RDD\",\"DataSet\",\"DataFrame\"],\"y\":[903.0,2202.0,652.0],\"type\":\"bar\"};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {};\n",
       " var config = {};\n",
       "\n",
       "  Plotly.plot('plot-9aceb670-0d57-4e83-83c8-ba9962712f67', data, layout, config);\n",
       "})();\n",
       "});\n",
       "      </script>\n",
       "           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mx\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"RDD\"\u001b[39m, \u001b[32m\"DataSet\"\u001b[39m, \u001b[32m\"DataFrame\"\u001b[39m)\n",
       "\u001b[36my\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m903\u001b[39m, \u001b[32m2202\u001b[39m, \u001b[32m652\u001b[39m)\n",
       "\u001b[36mres13_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-9aceb670-0d57-4e83-83c8-ba9962712f67\"\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (x, y) = Seq(\n",
    "    \"RDD\" -> runWithOutput(infectionAvgRDD.collect),\n",
    "    \"DataSet\" -> runWithOutput(avgDS.collect),\n",
    "    \"DataFrame\" -> runWithOutput(infAvgOrDf.collect)\n",
    ").unzip\n",
    "\n",
    "Bar(x, y).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
