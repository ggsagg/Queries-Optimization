{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimización de Consultas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creamos la sesión de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` \n",
    "//import $ivy.`sh.almond::almond-spark:0.6.0`\n",
    "\n",
    "import org.apache.spark.sql.{NotebookSparkSession, SparkSession}\n",
    "\n",
    "val spark: SparkSession = \n",
    "    NotebookSparkSession\n",
    "      .builder()\n",
    "      .appName(\"Queries Optimization\")\n",
    "      .master(\"local[*]\")\n",
    "      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JM: la ejecución de la celda anterior te devolverá el enlace a la Spark UI (al final del todo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.LoggerFactory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.slf4j.LoggerFactory\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getRootLogger().setLevel(Level.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.sqlContext.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{IntegerType, StringType, StructType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.{SparkConf, SparkContext}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => func, _}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._, func._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import spark.sqlContext.implicits._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructType}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._, func._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset ha sido obtenido de:\n",
    "https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide\n",
    "\n",
    "En el se observan los casos diarios de Covid-19 por país hasta el 14-12-20\n",
    "\n",
    "En la segunda parte se utilizan los datos de las medidas aplicadas a cada país por fecha de inicio y fin:\n",
    "\n",
    "https://www.ecdc.europa.eu/en/publications-data/download-data-response-measures-covid-19\n",
    "\n",
    "La última consulta para calcular las infecciones por km2:\n",
    "\n",
    "https://www.kaggle.com/tanuprabhu/population-by-country-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo una clase para trabajar con infecciones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JM: Creo que el error al que te refieres cuando utilizas dataset se debe a que no tienes esta primera línea en la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mInfection\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Infection(day : Int, \n",
    "                     month : Int, \n",
    "                     year : Int, \n",
    "                     nCases: Int, \n",
    "                     nDeaths : Int, \n",
    "                     country : String,  \n",
    "                     continent : String) \n",
    "extends Serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y un método para medir tiempos de ejecución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  JM: este método está ya implementado en SparkSession: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L676 de todas formas, el dato más preciso lo encontrarás en la Spark UI. Investiga si es posible acceder a ese dato desde alguna api de spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run[A](code: => A): A = {\n",
    "    val start = System.currentTimeMillis()\n",
    "    val res = code\n",
    "    println(s\"Took ${System.currentTimeMillis() - start}\")\n",
    "    res\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empiezo trabajando con RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mreadFromFile\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = data.csv MapPartitionsRDD[5] at textFile at cmd10.sc:1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val readFromFile = spark.sparkContext.textFile(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una funcion para trabajar con un RDD de infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minfections\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infections(lines : RDD[String]) : RDD[Infection] =\n",
    "    lines.map(line => {\n",
    "      val arr = line.split(\",\")\n",
    "      Infection(\n",
    "        day = arr(1).toInt,\n",
    "        month = arr(2).toInt,\n",
    "        year = arr(3).toInt,\n",
    "        nCases = arr(4).toInt,\n",
    "        nDeaths = arr(5).toInt,\n",
    "        country = arr(6),\n",
    "        continent = arr(10)\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo la media de infecciones diarias por país"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36minfectionGrowthAverage\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  def infectionGrowthAverage(infections : RDD[Infection]) : RDD[(String, Int)]= {\n",
    "\n",
    "    val countriesAndCases : RDD[(String, Iterable[Int])] = \n",
    "      infections.map(x => (x.country,x.nCases))\n",
    "      .groupByKey()\n",
    "      \n",
    "    countriesAndCases.mapValues(x => (x.sum / x.size)).sortBy(_._2)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestro el resultado y el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minfectionRDD\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInfection\u001b[39m] = MapPartitionsRDD[3] at map at cmd6.sc:2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val infectionRDD = infections(readFromFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infectionRDD.toDF.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JM: el tiempo de creación del RDD no es interesante; lo que quieres saber es el tiempo que tarda en _ejecutarse_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "var comm = Jupyter.notebook.kernel.comm_manager.new_comm('cancel-stage-d5d3dec8-8b84-410e-9bd0-d84f605ba936', {});\n",
       "\n",
       "function cancelStage(stageId) {\n",
       "  console.log('Cancelling stage ' + stageId);\n",
       "  comm.send({ 'stageId': stageId });\n",
       "}\n",
       "</script>\n",
       "          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">map at cmd12.sc:4</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    2 / 2\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">sortBy at cmd12.sc:7</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    2 / 2\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3778 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36minfGrAvRDD\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = MapPartitionsRDD[13] at sortBy at cmd12.sc:7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val infGrAvRDD = spark.time(infectionGrowthAverage(infectionRDD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">sortBy at cmd12.sc:7</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    2 / 2\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd14.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    2 / 2\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 472 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres14\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Greenland\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"British_Virgin_Islands\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"\\\"Bonaire\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Fiji\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Vanuatu\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Saint_Vincent_and_the_Grenadines\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Solomon_Islands\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Holy_See\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Dominica\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Wallis_and_Futuna\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Grenada\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Marshall_Islands\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Anguilla\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Timor_Leste\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Saint_Kitts_and_Nevis\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Falkland_Islands_(Malvinas)\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Montserrat\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Northern_Mariana_Islands\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Seychelles\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Antigua_and_Barbuda\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Laos\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Brunei_Darussalam\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"New_Caledonia\"\u001b[39m, \u001b[32m0\u001b[39m),\n",
       "  (\u001b[32m\"Guernsey\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Saint_Lucia\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Cayman_Islands\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Bermuda\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Isle_of_Man\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Barbados\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Cambodia\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Faroe_Islands\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Bhutan\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Monaco\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Mauritius\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"United_Republic_of_Tanzania\"\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m\"Papua_New_Guinea\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"Eritrea\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m\"Taiwan\"\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.time(infGrAvRDD.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hago los mismos calculos con un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convierto el RDD obtenido previamente en un DataFrame para inferir la clase infección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDF = spark.createDataFrame(infectionRDD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizo los métodos de la clase DF que incluye uno optimizado para calcular la media.\n",
    "\n",
    "Ejecuto y comprabamos como el tiempo de ejecución es significativamente menor que en RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infAvgOrDF = infectionDF.\n",
    "    groupBy(\"country\")\n",
    "    .avg(\"nCases\")\n",
    "    .orderBy(desc(\"avg(nCases)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infAvgOrDF.showHTML()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time(infAvgOrDF.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.time(infAvgOrDF.collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JM: Está muy bien para entender bien qué es lo que está haciendo spark, pero no tenemos mucho espacio en la memoria, por lo que deberíamos centrarnos en las queries y los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción es crear el DataFrame directamente importando los datos pero deja de ser un DF de infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovid = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"covidworldwide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dfCovid.toDF.groupBy(\"countriesAndTerritories\")\n",
    "    .agg(mean(\"cases\")).orderBy(\"avg(cases)\")).show(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "puedo definir el esquema manualmente para crear el DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Defino el esquema manualmente pero podría verlo importando el csv y viendo como lo hace de base spark\n",
    "\n",
    "val schema = new StructType()\n",
    "    .add(\"dateRep\",StringType,true)\n",
    "    .add(\"day\",IntegerType,true)\n",
    "    .add(\"month\",IntegerType,true)\n",
    "    .add(\"year\",IntegerType,true)\n",
    "    .add(\"cases\",IntegerType,true)\n",
    "    .add(\"deaths\",IntegerType,true)\n",
    "    .add(\"countriesAndTerritories\",StringType,true)\n",
    "    .add(\"geoId\",StringType,true)\n",
    "    .add(\"countryterritoryCode\",StringType,true)\n",
    "    .add(\"popData2018\",IntegerType,true)\n",
    "    .add(\"continentExp\",StringType,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read\n",
    ".format(\"csv\")\n",
    ".option(\"header\",\"true\")\n",
    ".schema(schema)\n",
    ".load(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y con un DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".csv(\"covidworldwide.csv\")\n",
    ".as[(String,String,String,String,String,String,String,String,String,String,String,String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\n",
    "    infectionDS.groupBy($\"countriesAndTerritories\")\n",
    "    .agg(avg($\"cases\").as[Double])\n",
    "    .orderBy(\"avg(cases)\")\n",
    "    .count\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras opciones menos eficientes\n",
    "(Work in progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JM: No compila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infectionDS.groupByKey(p => p._7) //hace shuffling de los datos\n",
    "        .mapValues(p => p._5.toDouble)\n",
    "        .mapGroups((k,vs) => (???))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JM: para comparativas de eficiencia https://db-blog.web.cern.ch/blog/luca-canali/2018-08-sparkmeasure-tool-performance-troubleshooting-apache-spark-workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infectionDS.groupByKey(p => p._7) //mas eficiente con el reduce\n",
    "        .mapValues(p => p._5.toDouble)\n",
    "        .reduceGroups((acc,str)=> ???)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infGrowAvg = new Aggregator[]{ //utilizando un Aggregator\n",
    "    \n",
    "}.toColumn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intento trabajar con DataSet de infecciones Dataset[Infection] pero falla \n",
    "(Work in progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos dará error pues no se pueden crear datasets sin tipo de datos y no entiende las infecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minfectionDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mInfection\u001b[39m] = [day: int, month: int ... 5 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val infectionDS = spark.createDataset(infections(readFromFile)).as[Infection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mds\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mInfection\u001b[39m] = [day: int, month: int ... 5 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds : Dataset[Infection] = spark.createDataset(infectionRDD).as[Infection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesito importar los Encoders y explicitar el tipo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Encoders\n",
    "Encoders.product[Infection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{Encoder, Encoders}\n",
    "import org.apache.spark.sql.Encoders\n",
    "\n",
    "val dataset = spark.createDataset(infectionRDD)(Encoders.product[Infection])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A partir de aquí intento crear un DataSet de infecciones pero siempre obtengo el mismo error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infectionDS = spark.createDataset(infectionRDD).as[Infection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def infections(lines : Dataset[(String,String,String,String,String,String,String,String,String,String,String,String)]) \n",
    "                       : Dataset[Infection] = \n",
    "    lines.map(line => {\n",
    "      Infection(\n",
    "        day = line._1.toInt,\n",
    "        month = line._2.toInt,\n",
    "        year = line._3.toInt,\n",
    "        nCases = line._4.toInt,\n",
    "        nDeaths = line._5.toInt,\n",
    "        country = line._6,\n",
    "        continent = line._10\n",
    "      )\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infections(infectionDS).as[Infection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizo una segunda tabla y cruzo datos con RDD, DS y DF\n",
    "(De momento solo uso DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfMeasures = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"response_graphs_data_2021-04-15.csv\")\n",
    "dfMeasures.show\n",
    "dfMeasures.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfPopulation = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"charset\", \"UTF8\")\n",
    ".option(\"delimiter\",\",\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"population_by_country_2020.csv\")\n",
    "dfPopulation.show\n",
    "dfPopulation.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifico los datos de entrada para que el formato fecha se adecue al TimeStamp de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfCovidClean = dfCovid.select($\"*\",$\"dateRep\",translate($\"dateRep\",\"/\",\"-\").as(\"new-date\")).drop(\"dateRep\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hago una consulta de prueba para obtener la media solo de los casos en España"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spainCovid = dfCovid.select(\"dateRep\",\"cases\").where(\"countriesAndTerritories == 'Spain'\").toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(spainCovid.agg(avg(\"cases\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cruzo los datos con un Join y hago algunas consultas sencillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val megaDF = dfCovid.join(dfMeasures, $\"Country\" === $\"countriesAndTerritories\")//.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megaDF.select(\"cases\",\"deaths\",\"dateRep\",\"Response_measure\")\n",
    "    .where(\"countriesAndTerritories == 'Spain'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(dfCovid.join(dfMeasures, $\"Country\" === $\"countriesAndTerritories\")\n",
    "        .select(\"cases\",\"deaths\",\"dateRep\",\"Response_measure\")\n",
    "        .where(\"countriesAndTerritories == 'Spain'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo una consulta para calcular la media de infecciones por Km2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JM: renombra las columnas en origen para no tener que escribir \"Country (or dependency)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JM: Está bien esta consulta, compara resultados con los casos por número de habitantes también"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\n",
    "dfCovid.join(dfPopulation, $\"Country (or dependency)\" === $\"countriesAndTerritories\")\n",
    "        .select($\"Country (or dependency)\" as \"Country\",\n",
    "                $\"dateRep\" as \"date\",\n",
    "                $\"cases\",\n",
    "                $\"Land Area (Km\\u00b2)\",\n",
    "                $\"cases\" / $\"Land Area (Km\\u00b2)\" as \"infection Per Km\\u00b2\")\n",
    "        .groupBy(\"Country\")\n",
    "        .avg(\"infection Per Km\\u00b2\")\n",
    "        .orderBy(desc(\"avg(infection Per Km²)\"))\n",
    "        .show\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas y observaciones personales interesantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.select(\"dateRep\",\"countriesAndTerritories\",\"cases\").show //aplico los métodos de DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCovid.createOrReplaceTempView(\"covid\") //se pueden aplicar consultas SQL sobre DF\n",
    "\n",
    "spark.sql(\"SELECT * FROM covid WHERE countriesAndTerritories == 'Spain'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//filtrados sobre dataframes\n",
    "dfCovid.filter($\"continentExp\" === \"Asia\" || $\"continentExp\" === \"Europe\").sort($\"continentExp\".asc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
